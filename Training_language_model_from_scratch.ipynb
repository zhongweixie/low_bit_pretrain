{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vw3oAFgJwa1O"
   },
   "source": [
    "# Training a causal language model from scratch (PyTorch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_K3DzFcgwa1R"
   },
   "source": [
    "Install the Transformers, Datasets, and Evaluate libraries to run this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "cS5gOIh0wa1R"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.21.0)\n",
      "Requirement already satisfied: evaluate in /usr/local/lib/python3.10/dist-packages (0.4.3)\n",
      "Requirement already satisfied: transformers[sentencepiece] in /usr/local/lib/python3.10/dist-packages (4.44.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.13.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.24.4)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.5)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets) (2023.12.2)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.25.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (2023.12.25)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (0.4.4)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (0.19.1)\n",
      "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (4.24.4)\n",
      "Requirement already satisfied: sentencepiece!=0.1.92,>=0.1.91 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (0.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.2->datasets) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.2.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.33.0)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.24.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (23.2)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.4)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)\n",
      "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.4.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.25.0)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.4.4)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.0->accelerate) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.0->accelerate) (2023.12.2)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.0->accelerate) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.0->accelerate) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.0->accelerate) (4.12.2)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.6.3)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.3)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: triton==3.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.0.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate) (12.6.68)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2024.2.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "Reading package lists... Done\n",
      "Building dependency tree... Done\n",
      "Reading state information... Done\n",
      "\u001b[1;31mE: \u001b[0mUnable to locate package git-lfs\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets evaluate transformers[sentencepiece]\n",
    "!pip install accelerate\n",
    "# To run the training on TPU, you will need to uncomment the following line:\n",
    "# !pip install cloud-tpu-client==0.10 torch==1.9.0 https://storage.googleapis.com/tpu-pytorch/wheels/torch_xla-1.9-cp37-cp37m-linux_x86_64.whl\n",
    "!apt install git-lfs\n",
    "!cd /workspace/matmulfreellm\n",
    "!pip install -e ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Et_LRxF9wa1S"
   },
   "source": [
    "You will need to setup git, adapt your email and name in the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "qYIaiJl2wa1T"
   },
   "outputs": [],
   "source": [
    "!git config --global user.email \"zhongwei.xie@hotmail.com\"\n",
    "!git config --global user.name \"zhongweixie\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eZKuiPLNwa1T"
   },
   "source": [
    "You will also need to be logged in to the Hugging Face Hub. Execute the following and enter your credentials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ipywidgets\n",
      "  Downloading ipywidgets-8.1.5-py3-none-any.whl.metadata (2.3 kB)\n",
      "Requirement already satisfied: comm>=0.1.3 in /usr/local/lib/python3.10/dist-packages (from ipywidgets) (0.2.1)\n",
      "Requirement already satisfied: ipython>=6.1.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets) (8.21.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /usr/local/lib/python3.10/dist-packages (from ipywidgets) (5.9.0)\n",
      "Collecting widgetsnbextension~=4.0.12 (from ipywidgets)\n",
      "  Downloading widgetsnbextension-4.0.13-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting jupyterlab-widgets~=3.0.12 (from ipywidgets)\n",
      "  Downloading jupyterlab_widgets-3.0.13-py3-none-any.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets) (0.19.1)\n",
      "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets) (0.1.6)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets) (3.0.43)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets) (2.17.2)\n",
      "Requirement already satisfied: stack-data in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets) (0.6.3)\n",
      "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets) (1.2.0)\n",
      "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets) (4.9.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.3)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.2.13)\n",
      "Requirement already satisfied: executing>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.0.1)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.4.1)\n",
      "Requirement already satisfied: pure-eval in /usr/local/lib/python3.10/dist-packages (from stack-data->ipython>=6.1.0->ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from asttokens>=2.1.0->stack-data->ipython>=6.1.0->ipywidgets) (1.16.0)\n",
      "Downloading ipywidgets-8.1.5-py3-none-any.whl (139 kB)\n",
      "Downloading jupyterlab_widgets-3.0.13-py3-none-any.whl (214 kB)\n",
      "Downloading widgetsnbextension-4.0.13-py3-none-any.whl (2.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: widgetsnbextension, jupyterlab-widgets, ipywidgets\n",
      "Successfully installed ipywidgets-8.1.5 jupyterlab-widgets-3.0.13 widgetsnbextension-4.0.13\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "pG7DX-6Awa1T"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b9b61627b4540c6a37fa88b57b574b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "PsKCaAwswa1T"
   },
   "outputs": [],
   "source": [
    "def any_keyword_in_string(string, keywords):\n",
    "    for keyword in keywords:\n",
    "        if keyword in string:\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "APHlON3fwa1U",
    "outputId": "079de3fa-a3db-4d21-bc0d-a990b22a4976"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False True\n"
     ]
    }
   ],
   "source": [
    "filters = [\"pandas\", \"sklearn\", \"matplotlib\", \"seaborn\"]\n",
    "example_1 = \"import numpy as np\"\n",
    "example_2 = \"import pandas as pd\"\n",
    "\n",
    "print(\n",
    "    any_keyword_in_string(example_1, filters), any_keyword_in_string(example_2, filters)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.21.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.13.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.24.4)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.5)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets) (2023.12.2)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.25.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.2->datasets) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.2.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "0feZU4qQwa1V"
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "from datasets import Dataset\n",
    "\n",
    "\n",
    "def filter_streaming_dataset(dataset, filters):\n",
    "    filtered_dict = defaultdict(list)\n",
    "    total = 0\n",
    "    for sample in tqdm(iter(dataset)):\n",
    "        total += 1\n",
    "        if any_keyword_in_string(sample[\"content\"], filters):\n",
    "            for k, v in sample.items():\n",
    "                filtered_dict[k].append(v)\n",
    "    print(f\"{len(filtered_dict['content'])/total:.2%} of data after filtering.\")\n",
    "    return Dataset.from_dict(filtered_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ab4ece29eeb4388b8bc3434b3b4e088",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/5912 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IterableDataset({\n",
      "    features: Unknown,\n",
      "    n_shards: 5912\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# This cell will take a very long time to execute, so you should skip it and go to\n",
    "# the next one!\n",
    "from datasets import load_dataset\n",
    "\n",
    "split = \"train\"  # \"valid\"\n",
    "filters = [\"text\", \"meta\"]\n",
    "#data = load_dataset(f\"/siflow/cerebras/SlimPajama-627B/\", split=split, streaming=True)\n",
    "\n",
    "data = load_dataset(f\"/siflow/cerebras/SlimPajama-627B/{split}/chunk1/\", split=split, streaming=True)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from data_loader import *\n",
    "\n",
    "device = 'cuda:0'\n",
    "device_type = 'cuda'\n",
    "batch_size =64\n",
    "max_seq_len = 350\n",
    "DATA_CACHE_DIR = \"/siflow/cerebras/SlimPajama-627B/train/chunk1/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "iter_batches = partial(\n",
    "    iter_batch_func,\n",
    "    device=device,\n",
    "    batch_size=batch_size,\n",
    "    max_seq_len=max_seq_len,\n",
    "    data_cache_dir=DATA_CACHE_DIR\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "functools.partial(<function iter_batch_func at 0x148982b8f640>, device='cuda:0', batch_size=64, max_seq_len=350, data_cache_dir='/siflow/cerebras/SlimPajama-627B/train/chunk1/')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iter_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5fcda9853d5481889d211b1389dfa01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/5913 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: IterableDataset({\n",
       "        features: Unknown,\n",
       "        n_shards: 5913\n",
       "    })\n",
       "    valid: IterableDataset({\n",
       "        features: Unknown,\n",
       "        n_shards: 2\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This cell will take a very long time to execute, so you should skip it and go to\n",
    "# the next one!\n",
    "from datasets import load_dataset, DatasetDict\n",
    "\n",
    "split0 = \"train\"  # \"valid\"\n",
    "split1 = \"validation\"\n",
    "filters = [\"text\", \"meta\"]\n",
    "\n",
    "ds_train = load_dataset(f\"/siflow/cerebras/SlimPajama-627B/{split0}/chunk1/\", split=split,streaming=True)\n",
    "ds_valid = load_dataset(f\"/siflow/cerebras/SlimPajama-627B/{split1}/chunk1/\", split=split,streaming=True)\n",
    "raw_datasets = DatasetDict(\n",
    "    {\n",
    "        \"train\": ds_train,  # .shuffle().select(range(50000)),\n",
    "        \"valid\": ds_valid,  # .shuffle().select(range(500))\n",
    "    }\n",
    ")\n",
    "\n",
    "print(type(raw_datasets[\"train\"]))\n",
    "raw_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__abstractmethods__', '__add__', '__annotations__', '__class__', '__class_getitem__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getitem__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__iter__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__orig_bases__', '__parameters__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__setstate__', '__sizeof__', '__slots__', '__str__', '__subclasshook__', '__weakref__', '_abc_impl', '_distributed', '_effective_generator', '_epoch', '_ex_iterable', '_formatting', '_head', '_info', '_is_main_process', '_is_protocol', '_iter_pytorch', '_prepare_ex_iterable_for_iteration', '_prepared_ex_iterable', '_resolve_features', '_shuffling', '_split', '_starting_state_dict', '_state_dict', '_step', '_token_per_repo_id', 'add_column', 'batch', 'builder_name', 'cast', 'cast_column', 'citation', 'column_names', 'config_name', 'dataset_size', 'description', 'download_checksums', 'download_size', 'epoch', 'features', 'filter', 'from_file', 'from_generator', 'from_spark', 'homepage', 'info', 'iter', 'license', 'load_state_dict', 'map', 'n_shards', 'remove_columns', 'rename_column', 'rename_columns', 'select_columns', 'set_epoch', 'shuffle', 'size_in_bytes', 'skip', 'split', 'state_dict', 'supervised_keys', 'take', 'task_templates', 'version', 'with_format']\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, DatasetDict\n",
    "\n",
    "print(dir(raw_datasets[\"train\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IterableDataset({\n",
      "    features: Unknown,\n",
      "    n_shards: 5912\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(raw_datasets[\"train\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
      "Collecting zstandard\n",
      "  Downloading zstandard-0.23.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: numpy>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.24.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Downloading zstandard-0.23.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: zstandard\n",
      "Successfully installed zstandard-0.23.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install pandas zstandard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zstandard\n",
    "import json\n",
    "import io\n",
    "\n",
    "def decompress_zst_to_json(zst_file_path):\n",
    "    dctx = zstandard.ZstdDecompressor()\n",
    "    with open(zst_file_path, 'rb') as compressed:\n",
    "        with dctx.stream_reader(compressed) as reader:\n",
    "            text_stream = io.TextIOWrapper(reader, encoding='utf-8')\n",
    "            data_list = [json.loads(line) for line in text_stream]\n",
    "    return data_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import IterableDataset\n",
    "\n",
    "class JsonIterableDataset(IterableDataset):\n",
    "    def __init__(self, data_list):\n",
    "        self.data_list = data_list\n",
    "\n",
    "    def __iter__(self):\n",
    "        for data in self.data_list:\n",
    "            yield data\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "    \n",
    "    def __len__(self):\n",
    "        # 返回数据集的大小，如果未知，可以返回 None 或者一个估计值\n",
    "        return len(self.data_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_folder(folder_path):\n",
    "    all_data = []\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith('.zst'):\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            data = decompress_zst_to_json(file_path)\n",
    "            all_data.extend(data)\n",
    "    return all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_specific_files(folder_path, num_files):\n",
    "    all_data = []\n",
    "    for i in range(num_files):\n",
    "        #example_train_0.jsonl.zst\n",
    "        filename = f\"example_train_{i}.jsonl.zst\"\n",
    "        #filename = f\"example_holdout_{i}.jsonl.zst\"\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        if os.path.exists(file_path):\n",
    "            data = decompress_zst_to_json(file_path)\n",
    "            all_data.extend(data)\n",
    "        else:\n",
    "            print(f\"File not found: {file_path}\")\n",
    "    return all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from datasets import Dataset\n",
    "\n",
    "# 假设 `data_list` 是从 `.zst` 文件解压并解析后得到的 JSON 数据列表\n",
    "all_data = process_specific_files(f\"/siflow/cerebras/SlimPajama-627B/{split}/chunk1/\",1)\n",
    "valid_data = process_specific_files(f\"/siflow/cerebras/SlimPajama-627B/validation/chunk1/\",1)\n",
    "\n",
    "    # 创建数据集和数据加载器\n",
    "dataset = JsonIterableDataset(all_data)\n",
    "valid_dataset =JsonIterableDataset(valid_data)\n",
    "# data_list = decompress_zst_to_json('/siflow/cerebras/SlimPajama-627B/validation/chunk1/example_holdout_0.jsonl.zst')\n",
    "# dataset = JsonIterableDataset(data_list)\n",
    "dataloader = DataLoader(dataset, batch_size=32)\n",
    "print(type(dataloader))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5PuXgQkZwa1V",
    "outputId": "ae3bb810-b2ce-4166-ffdf-2ef01b572f08"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['repo_name', 'path', 'copies', 'size', 'content', 'license'],\n",
       "        num_rows: 606720\n",
       "    })\n",
       "    valid: Dataset({\n",
       "        features: ['repo_name', 'path', 'copies', 'size', 'content', 'license'],\n",
       "        num_rows: 3322\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset, DatasetDict\n",
    "\n",
    "ds_train = load_dataset(\"huggingface-course/codeparrot-ds-train\", split=\"train\")\n",
    "ds_valid = load_dataset(\"huggingface-course/codeparrot-ds-valid\", split=\"validation\")\n",
    "\n",
    "raw_datasets = DatasetDict(\n",
    "    {\n",
    "        \"train\": ds_train,  # .shuffle().select(range(50000)),\n",
    "        \"valid\": ds_valid,  # .shuffle().select(range(500))\n",
    "    }\n",
    ")\n",
    "\n",
    "raw_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting chardet\n",
      "  Downloading chardet-5.2.0-py3-none-any.whl.metadata (3.4 kB)\n",
      "Downloading chardet-5.2.0-py3-none-any.whl (199 kB)\n",
      "Installing collected packages: chardet\n",
      "Successfully installed chardet-5.2.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install chardet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "jsonl_path = '/siflow/cerebras/SlimPajama-627B/validation/chunk1/example_train_0.jsonl'\n",
    "\n",
    "# zst_path ='/siflow/cerebras/SlimPajama-627B/train/chunk1/example_train_0.jsonl.zst'\n",
    "# decompress_zst_to_json(zst_path, jsonl_path)\n",
    "encoding = 'big5'\n",
    "common_encodings = ['utf-8', 'ascii', 'latin1', 'iso-8859-1', 'utf-16', 'utf-32', 'gbk', 'gb2312', 'big5']\n",
    "# for encoding in common_encodings:\n",
    "#     with open(jsonl_path, 'r',encoding=encoding) as f:\n",
    "#         data = [json.loads(line) for line in f]\n",
    "#         print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: cchardet in /usr/local/lib/python3.10/dist-packages (2.1.7)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0mThe encoding of the file is: None\n"
     ]
    }
   ],
   "source": [
    "!pip install cchardet\n",
    "import cchardet\n",
    "def detect_encoding_large_file(file_path, sample_size=4096):\n",
    "    with open(file_path, 'rb') as f:\n",
    "        raw_data = f.read(sample_size)\n",
    "    result = cchardet.detect(raw_data)\n",
    "    encoding = result.get('encoding')\n",
    "    return encoding\n",
    "\n",
    "file_path = '/siflow/cerebras/SlimPajama-627B/train/chunk1/example_train_0.jsonl.zst'\n",
    "encoding = detect_encoding_large_file(file_path)\n",
    "print(f'The encoding of the file is: {encoding}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed: /siflow/cerebras/SlimPajama-627B/train/chunk1/example_train_0.jsonl.zst cannot be decoded with utf-8\n",
      "Failed: /siflow/cerebras/SlimPajama-627B/train/chunk1/example_train_0.jsonl.zst cannot be decoded with ascii\n",
      "Success: /siflow/cerebras/SlimPajama-627B/train/chunk1/example_train_0.jsonl.zst can be decoded with latin1\n",
      "èsC\u001f\u0005ÆåCÚí¿ÇêÖ2ÛÔ)dúüû¥\u0014\\ù\u0017;\u0017\u0017\u0013Þªß¯h\u001fÁQá>tsðò*Å`§ÑT?+ó÷à¿6÷Ï{dÚ\u0012\u0019\u000e·Ü\u001bÈýÉµT\f",
      "\u000eçYi\u0005R^¡íYwuÏZO¯>¹Ö\u0012uÂPÇ­tª\u000e©f¿\u0016ú¾S¨!We\n",
      "ZxxX@`Â\u0000\u000b",
      "\u001b\u000e\u0006\u0016\u001a\u0016\u001e",
      "ZUó_",
      "ÖîXbw¤\u000b",
      "u\u0005[:Ö\n",
      "dÚ\u00066ÿ¹á\n",
      "È\u0003\u0003;ì\u0004@¡Ôd\u0016\u001c",
      "§q@¦AÂ  \n",
      "Õ±½¾r\u0003¯v.º\u0013j5´Ñºäxñyö-7µ}ZêO\u001d",
      "+/Åpp4ÈÐ(\u000e¼\u000e\n",
      "The encoding of the file is likely: latin1\n"
     ]
    }
   ],
   "source": [
    "def try_encodings(file_path, encodings):\n",
    "    with open(file_path, 'rb') as f:\n",
    "        raw_data = f.read()\n",
    "    for encoding in encodings:\n",
    "        try:\n",
    "            decoded_data = raw_data.decode(encoding)\n",
    "            print(f'Success: {file_path} can be decoded with {encoding}')\n",
    "            print(decoded_data[:500])  # 打印前500个字符进行检查\n",
    "            return encoding\n",
    "        except UnicodeDecodeError:\n",
    "            print(f'Failed: {file_path} cannot be decoded with {encoding}')\n",
    "    return None\n",
    "\n",
    "# 常见的编码列表\n",
    "common_encodings = ['utf-8', 'ascii', 'latin1', 'iso-8859-1', 'utf-16', 'utf-32', 'gbk', 'gb2312', 'big5']\n",
    "\n",
    "# 文件路径\n",
    "file_path = '/siflow/cerebras/SlimPajama-627B/train/chunk1/example_train_0.jsonl.zst'\n",
    "\n",
    "# 尝试不同的编码\n",
    "encoding = try_encodings(file_path, common_encodings)\n",
    "if encoding:\n",
    "    print(f'The encoding of the file is likely: {encoding}')\n",
    "else:\n",
    "    print('No encoding worked. The file may not be text, or it may be encoded in an uncommon format.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.utils.data.dataloader.DataLoader'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "{'text': ['Keb\\' Mo\\' Event Time & Tickets\\nMature Level: Machine-set Level 1\\nThis event has been marked mature level 1 due to the following, \"cocker\".\\nKeb\\' Mo\\' in Albany\\n1 State St.\\nKeb\\' Mo\\'\\nBuy TicketsKeb\\' Mo\\'\\nMusicTuesday, December 10 • 7:30pm • $43, $53, $63\\nPart of the American Roots & Branches Series\\nKeb\\' Mo\\'s Jingle Bell Jamboree\\nThis year Keb\\' Mo\\' is celebrating his 25th Anniversary year of performing under the moniker, with the release of two albums, Oklahoma and Moonlight, Mistletoe, And You, Keb\\'s first ever holiday album released throughout his lengthy and successful career. Jingle Bell Jamboree, will feature the four-time Grammy-award winning contemporary Blues and Americana artist and his band performing songs from Moonlight, Mistletoe, And You and other favorites. Alicia Michilli will open the concert.\\nEveryone purchasing a pair of tickets will receive a copy of the new Christmas album!\\nWATCH KEB\\' MO\\'\\nOver the years, Keb\\' Mo\\' has cultivated a reputation as a modern master of American roots music through the understated excellence of his live and studio performances. From performing in front of President Barack Obama at the White House to the LA Forum with Garth Brooks, Keb\\' Mo\\' is keeping the Blues alive with the ability to demand the attention of broad audiences spanning the globe. B.B. King, Buddy Guy, the Dixie Chicks, Joe Cocker, Robert Palmer and Tom Jones have all recorded his songs while his list of collaborators include Bonnie Raitt, Jackson Brown, Cassandra Wilson, Amy Grant, Solomon Burke, and many others.\\nKeb\\' Mo\\' has racked up four GRAMMY awards with the most recent being a 2017 GRAMMY for \"Best Contemporary Blues Album,\" for TajMo, a collaborative effort with legendary bluesman Taj Mahal. He has also earned multiple Blues Music Awards for \"Album of the Year,\" \"Contemporary Blues Album of the Year\" and \"Contemporary Blues Male Artist of the Year.\" His masterful guitar playing has inspired leading instrument makers Gibson Brands to issue the Keb\\' Mo\\' Signature Bluesmaster acoustic guitar and Martin Guitars to issue the HD-28KM Keb\\' Mo\\' Limited Edition Signature model.\\nThe multi-talented artist has worked in TV and film as well, having written and performed the theme song for the smash sitcom, \"Mike & Molly,\" and acting as the music composer for TV\\'s \"Memphis Beat.\" He also played Robert Johnson in the 1998 documentary \"Can\\'t You Hear The Wind Howl,\" while also appearing on \"Touched By an Angel,\" and John Sayles\\' 2007 movie, \"Honeydripper.\"\\nPhilanthropy is high on the priority list for Keb\\' who has been involved with Playing For Change since it\\'s inception, a program created to inspire, connect and bring peace to the world through music. Additionally, Keb\\' is a celebrity mentor with the Kennedy Center\\'s Turnaround Arts program, a highly successful program began under the guidance of Michelle Obama where artists work as mentors, working with teachers, students, parents and the community to help build a successful arts education program.\\nEvent details at theegg.org! | Buy tickets at stubhub.com! | Buy tickets at vividseats.com!\\nat The Egg\\nTrixie Mattel\\nTommy Emmanuel, CGP\\nEnter the Haggis\\nSorry, you missed Keb\\' Mo\\' at The Egg.\\nDemand that The Egg gets added to the next tour!\\nYou missed Keb\\' Mo\\' at The Egg.\\nWe\\'re generating custom event recommendations for you based on Keb\\' Mo\\' right now!', \"Information On Submitting 'In Loving Memory'/Death Notice.\\nMary Ellen Pastore\\nMary Ellen Pastore, 62, of Norwalk, Connecticut, died on January 25. She leaves a sister in Forestdale.\\nEdward F. McCann Jr.\\nEdward F. McCann Jr. of Sandwich died on January 22. He was 90.\\nMichael J. Musto\\nMichael J. Musto, 73, of Mashpee died on January 26 at McCarthy Care Center in Sandwich from cancer. He was the husband of Susan J. (Labute) Musto, with whom he was married…\\nAnne Burk\\nAnne Rieg Burk, 88, of Mashpee died peacefully at Mt. Auburn Hospital after a long illness on January 24. She was the daughter of the late Nicholas and Anne Rieg.\\nNathaniel T. Thayer\\nWord has been received that Nathaniel Talbot Thayer, who had been living in Woods Hole for the last several years, died at home January 3. Mr. Thayer, 62, was an award-winn…\\nElizabeth I. Zimmerli\\nElizabeth I. Zimmerli died on January 19, at the age of 83, after a brief battle with glioblastoma brain cancer. She had lived in Falmouth for the last 30 years, where she …\\nLeo F. Kogut\\nLeo F. Kogut, 93, of Falmouth died on January 19 after a brief illness.\\nJohn P. Maheras Jr.\\nJohn Peter Maheras Jr., 86, of Falmouth died at home on January 18.\\nAnn R. Bosche\\nAnn Ruth (Kerrigan) Bosche, 93, of Bradenton, Florida, died January 3. She was a former resident of Falmouth and leaves family in Bourne.\\nSusan M. Wilson\\nSusan M. Wilson, who spent the majority of her life in Falmouth, died January 23 after a battle with cancer. She was 61.\\nLeonard R. Kilmer\\nLeonard Reid Kilmer of Pocasset, who had a career in medicine, died at home on January 19 at the age of 89.\\nPaul H. Leopold Jr.\\nPaul H. Leopold Jr. of Bourne died on January 16 at the age of 80.\\nDaniel Kirby\\nDaniel Kirby, who was born and raised in Falmouth, died in his sleep on December 31. He was 46.\\nEugene R. Cremins\\nEugene Redmond Cremins of Stoneham and North Falmouth died January 15. He was 91.\\nFrank R. Hampson\\nFrank R. Hampson, 93, of Pocasset died January 22.\\nRichard S. Armstrong\\nRichard Sweet Armstrong of Castine, Maine, died January 12. He was the former owner of Fiddler's Cove Marina in North Falmouth, spent a term as a selectman in Falmouth, and…\\nMadeleine I. Felker\\nMadeleine I. Felker, who had an active presence in Falmouth for many years as a writer, educator and member of community groups, died January 19 at Windsor Skilled Nursing …\\nLinell W. Grundman\\nLinell Marie (Wenzel) Grundman, a former resident who served on the board of selectman and finance committee, was a community activist, and a board member of Sandwich Town …\\nThomas W. Sullivan\\nThomas W. Sullivan of Newton, who summered in Megansett, died January 8 after a two-year battle with cancer. He was 70.\\nLois Mercado\\nLois Mercado of Appleton, Wisconsin, a former senior research assistant at Woods Hole Oceanographic Institution, died January 9 at the age of 92. Her last years were spent …\\nBrian S. Teixeira\\nBrian S. Teixeira of Falmouth died December 28 from complications of diabetes. He was 60.\\nJohn F. Graham\\nJohn F. Graham of Falmouth died January 7 at Cape Cod Hospital. Mr. Graham, 88, was known as Jack.\\nMary Ann Broadwater\\nMary Ann (McDermott) Broadwater, 88, died January 14. She was a resident at Falmouth Royal Nursing & Rehabilitation Center.\\nCarol M. Wenzel\\nCarol Marie Wenzel of East Sandwich, 68, died January 14.\\nHerbert W. Ellis\\nHerbert W. Ellis of Sagamore Beach died at home on January 13 at the age of 94.\\nJohn P. Kivlan\\nJohn Payton Kivlan of East Sandwich, an attorney and former prosecutor, died January 11 at South Shore Hospital in Weymouth. Mr. Kivlan, 77, died from injuries sustained in…\\nSuzanne C. Kelly\\nSuzanne C. Kelly of Chatham, who leaves family in Mashpee, died at home on January 13 at the age of 95. Ms. Kelly was also known as Susan.\\nMichael A. Shelton\\nMichael A. Shelton, 70, of Sandwich died November 30 at McCarthy Care Center in East Sandwich.\\nWaldemar J. Dymecki\\nWaldemar John Dymecki of Mashpee died at home on January 9, two days before his 92nd birthday.\\nWinslow S. Cobb IV\\nWinslow Smith Cobb IV of Hudson, Wisconsin, died unexpectedly on January 5. He was 58 and leaves family in Sagamore Beach.\\nMarteve E. Williamson\\nMarteve Eugenie Williamson, who was known as Marty, died January 12 while living in the Avita Memory Care community of Laurentide at Mashpee Commons. She was 90 and had bee…\\nJeffry L. Arnold\\nJeffry Lynn Arnold, 82, of Mashpee died January 11 following a long illness.\\nDaniel F. Dwyer\\nDaniel F. Dwyer of Sandwich died on Monday, January 9. He was 80.\\nNancy A. Dunlap\\nNancy Adams Dunlap died at her home in Sandwich on January 6. She was 95.\\nGail L. McNiff\\nGail Louise (Temple) McNiff of Bourne died at home on December 30. She was 80.\\nAlbertina Rebelo\\nAlbertina Rebelo of Falmouth died January 11. She was 86.\\nLloyd S. Beckett Jr.\\nLloyd S. Beckett Jr. of Falmouth died unexpectedly on Friday, January 6, in Bradenton, Florida.\\nMabel Ann C. Higgs\\nMabel Ann (Corey) Higgs, a native of Falmouth, died Sunday, January 8, in Novi, Michigan.\\nWalter C. Scanlon\\nWalter Carpenter Scanlon of Falmouth died at JML Care Center on December 17. He was 97.\\nErnest Oberg\\nErnest Oberg, 82, died December 31 at his home in Sarasota, Florida. He had been battling cancer for more than 10 years. He was a former resident of Falmouth.\\nKathleen E. Hall\\nKathleen Engemann Hall of Falmouth died on January 6. She was 73.\\nWilliam J. McKay Jr.\\nWilliam John McKay Jr. of Mashpee, who taught school in Falmouth in the early 1970s, died Thursday, January 5, after a long battle with multiple myeloma. He was 75.\\nJames E. McNiff\\nJames Edward McNiff of South Grafton, who leaves family in Mashpee and Falmouth, died on January 3. He was 94.\\nSami L. Perra\\nSami Leigh Perra, 27, a resident of Mashpee and previously of Falmouth, died unexpectedly on Tuesday, January 3.\\nJoan C. Priest\\nWord has been received that Joan Chandler Priest of Holbrook, formerly of Falmouth, died November 26. She was 85.\\nEileen Tantum\\nEileen Tantum of East Falmouth died December 31. She was 102.\\nMarjorie Trudeau And Norma Lyons\\nMarjorie (DeChristoforo) Trudeau, 99, of Mashpee died December 30.\\nRegina C. Kilday\\nRegina C. Kilday of Falmouth died December 28 at McCarthy Care Center in East Sandwich, after a lengthy illness. She was 62.\\nEsther R. White\\nEsther Ruth White, 90, died December 26 while under hospice care at JML Care Center in Falmouth. She had been a resident for the past few years at Atria in Falmouth.\\nCharles J. Atkinson\\nCharles J. Atkinson of Falmouth died December 31 at Beth Israel Hospital in Needham. He was 73.\\nLatest Upper Cape News\\nBoard Votes For Smaller Tax Increase In Falmouth\\nMashpee Planning Board Debates Solar Zoning Changes\\nMashpee Select Board Notes: Speed Limit Concerns Gain Momentum\\nTribal Preference At Mashpee Housing Project Requires Financial Contribution\\nSandwich's Town Attorney Weighs In On Charter Changes\\nRebuild Plans For Harbor Street Home Sparks Heated Debate, Denial\\nSubscribe to the Enterprise and enjoy print and online access\", \"Common Agricultural Policy\\nDG Environment\\nFarm-to-Fork\\nIntensive farming\\nPositive list\\nAnimal Advocacy and Protection\\nBadger Trust\\nCatholic Concern for Animals\\nOn Monday 19 December, the 15th Conference of Parties (CoP) of the Convention on Biological Diversity (CBD) came to an end with the adoption of the Kunming-Montreal Global Biodiversity Framework, a set of targets to reverse the trend of biodiversity loss and protect wildlife and ecosystems. We acknowledge the promising inclusion of One Health but regret the absence of strong commitments to address animal welfare in a holistic approach.\\nCoP15 Global Biodiversity Framework: promising targets for biodiversity but failing to recognise the link between animal welfare and conservation\\nThe Parties to the Convention on International trade in Endangered Species of Wild fauna and flora (CITES) held their 19th Conference of the Parties in Panama last month. Our Wildlife Programme Leader Nick Clark attended to lobby for the protection of animals and learn of the latest developments in the regulation of the international trade in wild animals.\\n250 species used in exotic pet trade receive stronger protections at CITES CoP19\\nOn June 3rd the Portuguese Presidency of the Council hosted an event co-organised by Eurogroup for Animals and AAP Animal Advocacy and Protection, to explore the risks posed by the exotic pet trade in the EU and the opportunities for better regulation.\\nThe unregulated exotic pet trade in the EU: a threat to health and biodiversity\\nIn December 2020, the European Parliament published a new report on the link between biodiversity loss and the increasing spread of zoonotic diseases. The report, requested by the European Parliament's Committee on the Environment, Public Health and Food Safety, aims at informing EU policy makers and introducing policy options to reduce risks originating from wildlife trade.\\nThe link between biodiversity loss and the increasing spread of zoonotic diseases presented in new EP report\\nThe European Commission has launched a new Knowledge Centre for Biodiversity: a one-stop shop for science-based evidence to protect the natural ecosystems that provide us with food, medicines, materials, recreation, and wellbeing.\\nCommission launches knowledge centre to reverse biodiversity loss and protect Europe's ecosystems\\nTwo main highlights of this year EU Green Week, occurred from 19 to 22 October, were the State of Nature report launch, which showed worrying trends in EU protected species and habitats, and the presentation of the Positive List system to regulate the exotic pet trade.\\nEU Green Week's agenda: the need for more protection for endangered species and an EU positive list for exotic pets\", \"Major General David F. Bice is a retired Inspector General of the United States Marine Corps, formerly stationed in Washington, D.C. Bice retired from active duty in 2007 after over 36 years of service.\\n\\nBiography\\nDavid Bice was born on September 9, 1945 in Zanesville, Ohio and enlisted in the Marine Corps in June 1968. He attended recruit training at the Marine Corps Recruit Depot, San Diego. He was commissioned through the Enlisted Commissioning Program in April 1969. He received his bachelor's degree from Pepperdine University, and he holds a master's degree in business from Central Michigan University. His formal military education includes the Infantry Officer Advanced Course at Fort Benning, Georgia, and the National War College, where he studied as a senior research fellow.\\n\\nBice has commanded Marines at every level from platoon to division. He served as a rifle platoon commander with 3rd Battalion, 1st Marines, 1st Marine Division in the Republic of Vietnam. He led companies in: 1st Battalion, 4th Marines, 3rd Marine Division; 3rd Battalion, 7th Marines, 1st Marine Division and 1st Tank Battalion, 1st Marine Division.\\n\\nDuring 1986-1988, he commanded 3rd Battalion, 8th Marines, 2nd Marine Division. He commanded the 9th Marine Regiment from 1992 to 1994. He commanded the 3rd Marine Division in Okinawa, Japan, from 1994-1995.\\n\\nFrom 1999-2001 he served as deputy commander of Marine Forces Europe. His assignments outside the Fleet Marine Forces have included series commander of Marine Corps Recruit Depot, Parris Island, South Carolina; enlisted promotions plans officer, Manpower Dept., HQMC; chief of European Division, J-5, The Joint Staff; commanding general of Marine Corps Base Hawaii; director of Marine Corps Staff, HQMC; deputy commander of NATO Joint Headquarters South Center, Larissa, Greece (the base closed in 2004); and commanding general of Marine Corps Base, Camp Pendleton, California.\\n\\nBice also served as an exchange officer with the Royal Marines (RM) and is a graduate of the Royal Marines Commando Course. He left the Marine Corps in 2002 after serving as the commanding general of Camp Pendleton (ironically this is the camp where he started his military career) to work as an international defense consultant. He returned to active duty in August 2004 in support of the Global War on Terrorism, serving as the Inspector General of the Marine Corps.\\n\\nHe returned to retired status in 2007 after over 36 years of active service.\\n\\nAwards and decorations\\nHis personal decorations include:\\n\\nReferences\\n\\nLiving people\\nUnited States Marine Corps generals\\nRecipients of the Navy Distinguished Service Medal\\nRecipients of the Legion of Merit\\nRecipients of the Gallantry Cross (Vietnam)\\nPeople from Zanesville, Ohio\\nRecipients of the Defense Superior Service Medal\\n1945 births\", \"One thing that you need to know that there are many cosmetic industries being that there are also many people who use the same products. Because of that it is essential that you choose the best beauty shop to eliminate the cases of using the wrong makeup. It is essential to note that using wrong makeup is dangerous because it can cause irritation and infections. Here are some of the factors that you should consider to get the best beauty shop.\\nThe first thing that you need to consider is their products. You find that we have many beauty products such as Kerastase shampoo, eyeliner, eye shadow, hair remover among many others. Therefore, it is necessary that you choose a beauty shop that has a wide range of products thus allowing you to choose the one that will meet your needs. Besides, you should also ensure that their products are approved by the legal organization as this means that they are genuine.\\nApart from that, you should also consider the services that they are offering. You find that buying beauty products is not the same as using them. One thing that you should confirm is if they can educate you on how you should use the product for the better outcome. Some of the shops also offer application services to their customers. Besides, you should also check if they offer delivery services as this will mean that you will get the package at your doorstep.\\nBesides, you should also consider the manufacturing dates of their products. One thing that you need to know that there are a lot of developments that are taking place in the makeup industry as they try to produce better products that can give quality outcome. For that matter, if you don't care about how current their products are, it will mean that you are still using old products which are dangerous. Because of that it will be better you choose a beauty shop with the latest product being that they are effective and less dangerous.\\nAlso, you should also consider the buying process. To start with, we have beauty shops that you will have to visit them physically to make your purchase. Besides, there are also online beauty shops which have a wide range of products. You find that purchasing from online beauty shop is the easiest because you will just have to place your order and wait for the delivery at your doorstep. All in all, you should choose a beauty shop with the best quality products and reasonable rates.\\nGenerally, in more than a single room of the house, you will find a mirror. To be sure that you are presentable as you leave your home, you ought to check yourself in the mirror. Everyday, you have to use a mirror, hence it is a functional item. You are advised to ensure that the decorative mirror you choose for your home is perfect. When in need of a decorative mirror in your home, deliberate on the factors discussed below to help you choose the best.\\nWhen looking for the right decorative mirror wall for your house, it is vital to contemplate on its function. As a result of fulfilling a basic use that you need, it is the reason why mirrors are said to be primarily functional. Thus, in considering what you need the mirror for, will assist you in deciding if it is simply a decorative mirror or a mirror. There are different shapes, sizes as well as styles of wall mirrors. If you need magnification, it is vital to find a mirror that is suitable. You will find round mirrors, large wall mirrors, even heated mirrors as well as antique mirrors and star shapes.\\nThe kind of mirror that you select, can also be determined by the placement. Most of the time, you will look for a mirror that will fit a particular wall in your home. With a mirror, you can make smaller spaces appear larger. The space you intend to hand your wall mirror will highly affect the best size of mirror that you select. In general, large wall can take mirrors that are large in size. In case you are looking for a bathroom mirror, you ought to look for one that will suit damp conditions. Otherwise, the right hallway or bedroom mirror to select is one that is more ornamental in style.\\nWhen in need of a mirror for your home, it is vital to deliberate on its style. It is vital to contemplate the interior design of your existing room, if you are looking for an existing room mirror. When you are considering existing dcor, you will find numerous ranges of mirrors that have frames that can be used to fit in the style of your home. You can make the mirror a design feature if you desire with a shape and size of the mirror that you pick for your home. When you utilize the above tips when selecting a mirror wall for your home, you will end up getting the best. Once you have purchased a perfect wall mirror, you are highly advised to hand it carefully and securely.\\nThe internet has become a market place for most businesses today. Business owners need to get the most reliable and fastest Web hosting and IT provider to address the IT requirements of the business. Many at times have small businesses failed to contract a company to manage their IT needs, and they end up doing it by themselves, failing every time because of lack of knowledge. The perfect IT service provider will see problems before they happen and prevent them from occurring, outclassing the large number of service providers willing to offer services after problems have happened. There are many IT required for all businesses, some of which will be explored in this paper. The points that will be discussed in this work will come in handy when choosing the right IT service provider.\\nThe server and workstation is the first areas that need maintenance. The server needs to be kept under observation and automatically updated with the necessary patches for the business to run smoothly. Your IT service provider should do that and should in addition set up backups to ensure that valuable information is always available. Extra protection may also be offered by checking and testing the integrity of backups. The other crucial maintenance services include cleaning of virus and malware, general troubleshooting and regular scans.\\nWhether you are operating a new or growing business, fast and ideal network setup and maintenance is important. Experts in IT know which is the perfect technology needed to provide the setup that your office requires. Wireless connection and networking is the new trend and with this technology you are guaranteed that all of your digital files will be stored in your server without causing any slowdowns. Your IT service provider should help in implementing this technology to be monitored remotely and offer maintenance to it every time.\\nYour IT provider must in addition offer email and software support. An office suite with an interface that is simple to use with all the email function is crucial. Your IT service provider must copy all the data on the old email if you plan to use a different platform. The type of software and email support that you will require should all be provided by the IT company. Some of the software related problems that the IT service provider should comfortably deal with include troubleshooting the antivirus and spam filtering.\\nIn conclusion, the IT company has to provide offsite and onsite emergency support fast and efficiently. IT issues must be dealt with speedily so that the business does not lose a lot of profits. With all that in mind, it is important you know where to start looking for an IT company like RemarkableTek that will offer value for all of your It requirements.\\nBenefits of Online College Courses for Credit.\\nThe number of students who are enrolling for online college classes has increased significantly over the past few years. Most of the colleges and universities are offering these classes for students nowadays. Besides this, most of these universities and colleges are accepting transfer credits these days and this is something that was never done there before. This is essential since it reaches a wider audience and they are able to learn from any part of the world.\\nMany people will question whether the quality of education is the same if the student takes their classes online while others have to learn from the school itself. However, there may be some difference in the length of time taken to complete the course by both parties. There are many benefits of taking online courses as highlighted in this article.\\nOne of the benefits of online learning is that the total costs are lower than in traditional schools. This shows that a greater number of students can afford these courses. There is so much advancement in education since some of the schools accept credits earned by students through the massive open online courses (MOOCs).\\nIn addition to this, taking online courses is convenient and flexible. The students are therefore able to plan their time well according to the activities they have and they also have enough time to learn. This is convenient enough since a student can work and learn after they are done with work.\\nIt is also possible for one to transfer their credits. For those students who cannot easily access their colleges because of one reason or the other, it is possible for them to transfer their credits to the primary college and this works to their advantage. This means they will be able to earn credits while they are attending to other duties that call for their attention.\\nOnline learning enables students to feel the comfort that comes with the environment they will be studying in. Here, one can study while on their pajamas and take the course from anywhere they like unlike what happens when they have to attend the physical classes. When a student takes their learning online, it means that they will do everything electronically and they will not have to worry about getting caught up in traffic when they need to attend classes or they want to spend time with their families.\\nThe advantages that have been highlighted here show how the online classes are beneficial to many students. The article has just highlighted some of the advantages yet there are many more that have not been mentioned here. Since there will be more thirst for education and many colleges are embracing online learning, the students should not shy away if they want to sign up for the classes. Online courses can be afforded by many students and they should therefore sign up for them.\\nReasons Why It Is Important That You Visit An Allergy Center.\\nNearly all people in the world have allergies. Allergies may prevent you from being yourself by how they alter your appearance. This can manifest in the form of itchy eyes, nose and throat, nasal congestions or running nose among others. Allergic reactions can make you very uncomfortable. It feels horrible to miss that adventure you planned with your friends because of allergic symptoms. A lot of people are ignorant about the cause of their allergies. When you can find a solution to your allergy problem, your life will be better and more comfortable than ever before. Going through this article will help you to know the benefits of visiting an allergy center.\\nYou stand to benefit from interacting with specialists who will diagnose your symptoms accurately. It is a pity that many people who suffer from allergies do not seek the services of allergists to get a comprehensive diagnosis but instead lean on the assumptions of the unqualified people around them. It is very difficult for one to tell the cause of their allergy symptoms through guessing. This is where medics in allergic centers come in. Such doctors can carry out tests and will determine ultimately the allergy symptoms you bear.\\nAllergy centers offer a variety of tests to their patients. You will, therefore, get to know the reasons for your suffering. The vastness of the tests you take will result in you getting to know other elements you are allergic to. You will hence be able to adjust your life accordingly to ease the allergy symptoms.\\nIt will be possible for you to access technological equipment that is more efficient. Going to an ordinary general hospital may not give you the same privilege. You should thus take advantage of such opportunity and enjoy more comfort when you are being checked.\\nWhen you visit an allergy center, the doctor will help you to interpret the results. You can therefore fully understand what exactly happens that you end up having symptoms.\\nGoing to an allergy center can help you get rid of the myths you have always believed in concerning allergies. Many men, women, and children have wrong ideas relating to allergies, and they need to find a correction. It is worthy for you to visit a specialist in allergy for you to clear all your questions regarding allergies. In this way, you will become more knowledgeable in the matters of allergies. Hence, you will be able to share your knowledge with individuals who have allergy issues.\\nBuying shares in penny stocks can be as successful as making an investment. While investment requires high skills and relevant experience, it can be hard for a new investor to make it. But through purchasing shares in penny stocks, one can make a profit and furthermore learn how to make a heavy investment solely in the future. Penny stocks offer the single opportunity of buying shares at a low cost. And with it you can progressively start to grow. But indeed before purchasing shares, you need to have the clear insight about it. This articles will furnish you with the most relevant factors that you need to consider before making your decision.\\nNormally, you will find that loss or success of this type of investment will depend on the type of penny stocks that one chooses. It is important to mention that some penny stocks are more likely to succeed than others. One of the facts that add odds of success to your penny stocks investment is your field of specialty as an investor. Many investors do lose because they have chosen unrelated investment to their careers. When looking for shares to buy, it is preferable to buy shares of your professionalism than any other different shares.\\nThere are possible bad consequences when you buy shares from new companies. Possibly; your account may remain dormant for a long period of time or those companies may even disappear from the market. And consequently, that will hinder your shares progress. Ideally, you should consider buying shares from those well-famed companies which have a good reputation and enough experience. If you are not sure which company to buy shares from, well, you can consider consulting your experienced friends. Friends with know-how will guide you to find the right company that you can buy shares from. You can also find those companies by conducting the online research, whereby you can even measure good companies by considering their customer's review.\\nIt is imperative to highlight that shares prices vary depending on their nature and the selling company. Some companies are more profitable than others. Telecommunication shares, for example, will not have the same shares price as the banking company shares. So will it be the healthcare shares versus the manufacturing company's shares. Practically, you will arrange your budget conveniently after, identifying and determining the types and nature of shares worthy for you. You can visit many penny stock exchanges near you to see how shares prices differ and ask all the questions you might have.\\nRecovering patients are now getting chiropractic care and physical therapy management after their hospitalization. Doctors now realize the importance of allowing their patients to become physically active after being treated in the hospital. Doctors and physical therapists are now working side-by-side to help patients become rehabilitated. The last two decades have seen a boom in the industry of chiropractic care and physical management due to increasing demand by customers and patients. More and more people now understand the benefits of getting physical therapy after getting surgical operations or after being treated in the hospital for other concerns. Having physical therapy will allow our body to heal naturally via the improvement of blood circulation.\\nMany healing benefits can be found in chiropractic. Movement of the body stimulates regeneration and recovery. Right now, many problems concerning musculoskeletal dysfunction can now be resolved through the use of chiropractic techniques. These techniques can be used for pain relief of chronic problems to physical rehabilitation of movement after spinal injuries. Some athletes also do physical therapy in order to improve performance. Physical therapy can also improve the body's immune function and slow down the process of aging. Benefits from alleviation of spine problems, neck and back pain can be derived from chiropractic care. Athletes and many professional players always turn to chiropractic care and physical therapy to recover quickly from injury during a game.\\nMany people believe that staying in bed after getting in a car accident or for other reasons will improve people's health much quicker. However, much of the literature right now prohibits people from being confined too long in hospitals or in bed due to the detrimental effects it will have to patients. It is now clear that getting physical activity helps people recover much faster after their injuries. Staying immobile and in bed has right now been proven to be a definite risk factor for other problems. Certain amount of physical activity has been proven to keep people safe from acquiring more severe health problems. Problems such as pressure ulcers and vascular thrombosis are an ever present hazard to people who are constantly bed ridden or unwilling to move. That is why many doctors now advocate physical therapy to people who can manage to move and control their own body. Our body is designed for movement and action, that is why the faster we can get back to moving, the quicker we heal from our problems. Physical therapy have been found to be effective for the circulation of the body. Pain tolerance and body homeostasis also improve. Chiropractic training and physical therapy isn't as hard as most people think. There are now a lot of reliable practitioners out there that can give you their service.\\nMany people find unwanted hair uncomfortable. It can tamper with an individual's self-esteem and create a casual I impression. Hence, many individuals search for the best means of getting rid of unwanted hair growth. One can get a shave or even attend waxing sessions. Others go for methods such as plucking. Some of these methods are not efficient since the hair will grow after a short time. Waxing or plucking can cause excruciating pain and irritate the skin surface.\\nPeople should find other alternatives to traditional hair removal methods; one can opt for laser technology. The laser hair removal treatment is one of the best since it stops and slows down hair growth. The hair will stay for long without growing thus an individual can maintain neatness. The technology is all about laser beams that heat the hair follicle and stop or slow down growth. Here are the benefits of using laser hair removal services.\\nUsing laser hair removal technique is convenient and can aid an individual in getting intended results with a few treatments. After a few weeks of treatment, one can stop or slow down hair growth efficiently. Unlike other methods, laser hair removal technique can work on all body parts without creating discomfort. The technique does not irritate the surrounding skin surface and has few side effects that do not last for long.\\nIf a person decides to use methods such as plucking, waxing or shaving, it will neither stop or slow down hair growth. Therefore, a person has to visit a specialist such as barbers frequently to stay tidy; it leads to a lot of expenses. Laser technology promote long term remedies as it stops or slows down hair growth and one can take control over their spending. The method can successfully prevent hair growth and make sure an individual maintains a smooth skin surface.\\nOther methods like plucking and waxing can cause an individual a lot of pain. They can tamper with the skin surface around the hair and irritate. Laser hair removal method is different as one does not experience pain. It aids individuals to get rid of unwanted hair.\\nThe laser technology can get rid of hair all over the body. It is efficient for people with a dark skin tone since the surface absorbs the laser beam fast. One can apply the method on sensitive parts such as a scar. When making preparations for a session, an individual should shave to boost the outcomes. One should consult a dermatologist if they have a problem with their skin.\\nYou find that purchasing a mattress can be tiresome and confusing being that there are many mattresses in the market. Besides, there are also some of the things that you should know before you start shopping for a mattress. Here are some of the tips for choosing the right mattress for your needs.\\nFirst of all, you should check the durability of the mattress. One thing that you should know is that a mattress is one thing that you can use for a long time and you should make sure that it is durable and it will be comfortable for a long time. You find that the durability of the mattress is always determined by the quality of materials that have been used to make it. Where you will have to go with some reputable brands that are known for the best quality. Not only that but you can also choose handcrafted mattresses which are known for their durability.\\nAnother thing that you should check is the size of the mattress. Where you will have to make sure that the size of mattress that you have chosen can comfortably fit in your bed or space where you are going to use it. Besides, it is also necessary that you choose a mattress that can move freely on the bed. When choosing mattress size it is necessary that you consider the heights of the people who will be using the mattress.\\nBesides, you should also consider motion disturbance. One thing that you need to know that when you are sharing a bed with someone they might interfere with your sleep while turning or tossing on the bed. In this case, you should go for a mattress which will mould to your movements and also absorb shock. This way you will be in a position to enjoy your sleep without interference.\\nApart from that, you should also consider firmness. One thing that you should know is that firmness of a mattress can be determined by your choice as some people prefer a soft mattress. Preferably, you should go for a mattress that shows a medium firm as this will prevent you from suffering back pain. On the other hand, when the mattress is too firm and you don't have enough weight to compress it, you will not get the comfort that you need.\\nBesides, it is also essential to check the warranty. In this case, you should choose a mattress that comes with a written warranty. One good thing with a warranty is that you will have your mattress replaced so long as the warrant is still active when you find the mattress to be faulty.\\nIf you want to sell your house, you would not want it to stay in the market for long. Some people sell their houses due to work transfers, upgrading, or downsizing. Therefore, there is a need for everyone selling a house to do the necessary to find a buyer as soon as possible. You should note that there are various buyers, as well as many people selling their houses. You should note that buyers are most likely to show an interest in your house if it is attractive. Most houses stay in the market for a long time because the owners do not know what to do in order for buyers to buy them. It would not be hard for you to sell your house if you follow the tips below.\\nYou will not experience much stress if you choose to work with an agent. It is for a fact that the agent has what it takes to secure you a good deal. Thus, you will be able to save a considerable amount of time and money. Selling a house for the first time requires you to use an agent because you may not know what is required. Ensure that you choose the appropriate agent to help you throughout the process. Consider the experience and qualifications of the agent before entering into a contract. You should also consider the ways that the agent would use to market your house.\\nYou are most likely to sell the house faster if you upload professional videos and photos of the house you are selling. The first thing a house hunter thinks of doing when in need of a house is an online search. This, it is necessary to ensure that the photos and videos you upload are professional. Moreover, the photographer knows the best time to take the pictures; thus, you would not regret your decision. Before hiring a photographer, it is necessary for you to see previous videos and photos to ensure that you choose the right person.\\nIt is advisable to depersonalize the house. There is no need to start selling your house when family photos and other collectibles are there. It is easier for a buyer to buy a house if he or she can imagine himself or herself living in that house together with the family. It would be easier to find a buyer if you keep the house simple and clean.\\nEnsure that you price the house correctly. Do not price the house before knowing the market price. Ensure that you do the right thing in order to find a buyer for your house.\", 'What destroyed Detroit is now destroying America\\nby John Couretas • June 2, 2020\\nWhen I first moved to Grand Rapids, Michigan, in 1986, the city was an alien place to me. I had grown up on the eastern side of the state, in the I-75 manufacturing corridor that runs from Toledo to Bay City. Soon, I came to realize that in Grand Rapids, I wasn\\'t just living in a different region of Michigan: I was living in a different state, a different culture. It was shocking to hear people in West Michigan crow about the problems in Detroit and other cities to the east.\\nThey were doing much better in the western part of the state, they told me. They didn\\'t have the corrupt political machines, the trade union stranglehold on vast swaths of the economy, the crime waves, the once beautiful neighborhoods reduced to ruins.\\nThe boosterish claims for Grand Rapids and the critiques of everything gone wrong in the Motor City were, to my ears, arrogant and unjustified. Over time though, I came around. I watched the continuing decline of the state of East Michigan and the growth of the state of West Michigan. They became two states headed in opposite directions.\\nDetroit and much of southeast Michigan was going through a bad patch in the 1980s and 1990s. The once-mighty auto companies were being humiliated by foreign competition. Labor unions accelerated the decline with a death grip on their privileges, and cities like Detroit, Pontiac, Flint, and Saginaw were hollowed out. They were in ruins. Photographers traveled from all over the world to shoot the shocking state of these cities. At the same time, the growth of China, Mexico, and other globalized manufacturing bases also meant that these cities would never recover their glory days—at least not in the lifetimes of those old enough to remember them.\\nBut there was a much more potent factor in the decline of those cities, far more powerful than anything related to globalization, trade, or manufacturing could accomplish. The July 1967 race riots that tore through Michigan cities caused immediate death and destruction, but the damage lasted for decades. Much has been written about white flight from these cities after the riots, and that\\'s true. What gets less attention is the black flight of small business owners, teachers, and other professionals who left the city for the suburbs with their families.\\nGrand Rapids had race riots in 1967, too, but its response was different. The city\\'s business and philanthropic class began pouring hundreds of millions of dollars of their own money—sometimes combined with public funds—into high-rise office buildings, hotels, a convention center, an arena, a massive hospital, and a medical research district. A brand-spanking-new downtown campus of Grand Valley State University rose up within walking distance. A central business district that was moribund in the mid-1980s came back to life in a way that East Michiganders marveled at. Most remarkably, as investors poured millions into new condos and apartment buildings, people started moving downtown.\\nAll of this was happening in a smallish Midwestern manufacturing city of about 200,000 people. In East Michigan, many cities were simply abandoned. All you saw were rotting factories and abandoned offices. Companies simply walked away and left the ruins behind.\\nThis week, Grand Rapids showed that it really isn\\'t any better or worse than Detroit. I may have to change my views again. Watch this report:\\nDid these Grand Rapids looters and arsonists miss the 2015 study that linked growth in African-American entrepreneurship to a decline in black youth violence between 1990 and 2000? A news report on its findings notes:\\n[B]lack-owned businesses act as \"social buffers\": their owners serve as role models to young people and create social networks that shield and divert youth people from a life of crime. Another reason is that black businesses mitigate some of the economic factors that contribute to youth violence in these communities. They add jobs, provide employment opportunities, and generally improve the neighborhood.\\nBut starting businesses in these communities isn\\'t easy. African Americans often don\\'t have the same kind of access to small-business loans as other racial groups. Parker says that given the positive effect African American businesses have on their communities; it might be time for a shift in policy focus. Last week, for instance, Baltimore mayor Stephanie Rawlings-Blake urged the city\\'s black community to \"step up\" and help put an end to \"black on black crime.\" What Parker\\'s study suggests is that cities themselves can step up to this task by supporting black entrepreneurs.\\nSupport black entrepreneurs? The principle means of supporting entrepreneurs, and all business owners, is for governments to do their job: to protect life, liberty, and property and to establish the secure conditions that allow working people and business owners to earn a living. That didn\\'t happen in Grand Rapids this week–and there were dozens more stories of arson, looting, and destruction that reporters never reported.\\nThe morning after the Villa Clothing Store was looted, local volunteers did a good deed by showing up downtown with brooms and buckets to help clean up the damage. Good on them. But the damage to Grand Rapids from these riots will not be swept away the morning after.\\nUnless order is restored immediately, and Grand Rapids residents can be assured that there will be no repeats of this week\\'s depredations in the long term, here\\'s what to expect: plummeting property values; a flight to safer environs by downtown business owners, residents, and workers; and a return to a moribund city center. The hundreds of millions of dollars invested in the downtown will be a write-off.\\nNow, the people of the city of Grand Rapids must decide how they want to go forward. Those who temporarily hold elective office or top administrative jobs should be replaced if the disorder returns.\\nThe first article of the Michigan Constitution holds that \"all political power is inherent in the people. Government is instituted for their equal benefit, security, and protection.\" Those we elected to operate the machinery of government utterly failed this week. For many in Grand Rapids, there was little or no security or protection.\\nNo one is arguing against the right to peaceful protests over George Floyd\\'s death. Peaceable assembly and orderly protest is the American way. But the civil authorities must restore order. That\\'s job number one. Otherwise, at least in Grand Rapids, they\\'ll have to stop crowing about how much better things are here than in Detroit.\\n(Photo credit: Rioters shattered the door of the Grand Rapids Art Museum. ABPhotog / Shutterstock.com. This photo has been cropped.)\\nJohn Couretas\\nis a writer and editor based in Grand Rapids, Michigan.\\nPosted in News and EventsTagged 1967 race riots, African-American neighborhood, detroit, George Floyd, grand rapids, long hot summer 1967, looting, Race Relations\\nPresident Trump visits Grand Rapids, promises to turn it into Detroit\\nDetroit, Urban Development, and D.G. Hart\\nWSJ cites Rev. Sirico in \\'A Requiem for Detroit\\'\\nShoeing Horses in Detroit: How Unions Are Hindering A City\\'s Revival', 'Ocean acidification has a particularly strong impact on miniscule zooplankton, down at the bottom of the food chain, as well as for fish and fisheries. (Photo: iStockphoto)\\nOcean acidification puts Norway in a pickle\\nResearch since the last IPPC climate change report has provided further proof of mounting acidification of the oceans. Norwegian experts think the fifth IPPC assessment report, to be released in Stockholm this week, should be an eye-opener.\\nmonday 30. September 2013 - 06:01\\nDebates about carbon emissions and the consequences for human life don\\'t usually encompass direct ways that Norwegians will be affected.\\nRising sea levels that will engulf the coasts of Bangladesh, the disappearance of South Pacific islands and more extreme hurricanes in the Gulf of Mexico are good reasons to curb carbon emissions, but they don\\'t radically change the lives of Scandinavians, at least short-term.\\nBut since the UN\\'s 2007 climate report was issued, research has reinforced our understanding of ocean acidification, a problem that has immeasurable momentum. It also can undermine Norway\\'s seafood industry.\\nOcean acidification is especially bad for marine organisms that make shells out of calcium carbonate. Among these are numerous types of plankton, prawns, lobsters, snails, mussels, starfish, sea urchins and corals.\\nProfessor Eystein Jansen. (Photo: Andreas R. Graven)\\nAcidification occurs when ocean water (and freshwater lakes) absorb atmospheric carbon dioxide, some of which reacts with water to form carbonic acid.\\nThe result is that the ocean\\'s pH, although still basic rather than acidic, is getting lower, or more acidic. Cold water absorbs more CO2 than warm water, and freshwater from melting ice can curtail the ocean\\'s ability to neutralize this acidification.\\nThis means that ocean acidification has particularly strong consequences for cold coastal countries like Norway.\\nPotential problem\\n\"Clearly, ocean acidification is a problem for a nation that is heavily involved in fisheries. Verifiable research now shows that the ocean\\'s pH value has changed, and this makes the polar areas the most vulnerable,\" says Eystein Jansen of the Bjerknes Centre for Climate Reseach, one of the main authors of the new climate report to be issued on Friday.\\nArne Johan Vetlesen is a professor of philosophy at the University of Oslo. (Photo: Didrik Søderlind)\\nAccording to the Norwegian Environment Agency, the surface of the ocean has become an average of 30 percent more acidic worldwide in the past 200 years.\\nIn July this year the concentration of carbon dioxide in the atmosphere reached a record-breaking 400 ppm (parts per million) at the Norwegian Institute for Air Research\\'s station on Svalbard. Even if the Earth\\'s mean temperature has not increased in the past 10-15 years, carbon dioxide levels continue to mount, and the oceans are being acidified.\\nNot insignificant\\nJansen calls ocean acidification the greenhouse effect\\'s evil twin. No matter what we do to mitigate the effect of emissions, ocean acidification will continue as long as we emit carbon dioxide.\\n\"If a new report is much clearer and assertive with regard to the sea, and also on the impacts on Norway, Norwegian politicians will have to take this much more seriously than earlier assessments,\" says Arne Johan Vetlesen of the University of Oslo.\\nHe is a philosopher and avid climate debater and author who writes about environmental issues.\\nVetlesen points out that Norwegians drive cars the most and use collective transport the least in comparison with other European countries. Norwegians are also near the top of the list worldwide when it comes to vacation air travel.\\n\"I think politicians have to face the music. But to date we\\'ve had some sort of artificial lag or delay in public discourse on Norwegians\\' consumption as part of the problem. We are not a role model. On the contrary we could serve as a warning, a bad example.\"\\nScallops in acidified waters\\nSissel Andersen of the Institute of Marine Research has just completed a study of what happens to scallops when the pH of the sea drops. Her findings will be published this autumn.\\nScallop larvae are important food for zooplankton, which in turn are eaten by fish. The shells are also important because they bind carbon in the sea.\\nIn her experiments scientists fertilized scallops and released their tiny larvae in tubs of water with varying degrees of alkalinity to see whether this had an effect on them in their first days of life. This is when their shells are most vulnerable to lower pH values.\\nThe tiny larvae create a thin shell within just a few days, making them look like microscopic scallops.\\nThese thin shells are composed of calcium carbonate and scientists checked to see if the lower pH values in the seawater caused shell deformations or lower survival rates.\\nThe pH level of normal seawater in Norway can now be as low as 7.98. The researchers filled a tub with seawater with a pH of 7.7, which is what they think we can expect in the ocean in 100 years if we keep emitting CO2 like we do today.\\nThe larvae in pH 7.7 seawater were less likely to survive and likelier to have shell deformities than the scallop larvae in the control group, which were raised in water with a pH of 7.98.\\n\"The results show that acidification has a big impact on survivability and growth. This is typical for many types of ocean larvae,\" says Andersen.\\nDoesn\\'t affect fish directly\\nOther researchers at the Institute of Marine Research have conducted similar studies of cod larvae, but these studies showed that variations in pH of the water had no impact on survivability.\\nEven though fish might not be directly impacted, changes in the food chain can be perilous, according to marine biologist Jan Helge Fosså of the Institute of Marine Research.\\n\"Species react very differently to changes in pH values. This can transform the competition between species. In other words, new species could take a dominant position in the food chain. New food chains can be started. But there\\'s no way to predict the outcome.\"\\nImportant management\\nFosså says that previous research shows that changes in environmental conditions and overfishing are a recipe for causing fish stocks to collapse.\\n\"The best advice is to manage our fish stocks as well as we can. We cannot fish out local stocks and we should try to promote as much genetic variation as possible. Maybe the species can adapt, but we don\\'t know which groups in a stock have the best chances of adaption and survival.\"\\nHe says that even though Norway ranks high internationally with regard to fisheries management, much hinges upon EU countries doing their jobs in the North Sea and continued smooth cooperation with Russia in the Barents Sea.\\nRead the Norwegian version of this article at forskning.no\\nArne Johan Vetlesen\\' profile\\nProfessor Eystein Jansen\\'s profile\\nArctic waters growing alarmingly acidic\\nIn the past 200 years the average acidity of surface waters in the world\\'s oceans has risen by 30 percent. This is prime evidence of humans really changing the entire planet.\\nPreviously unknown particle increases ocean acidity\\nUntil recently, nobody knew tiny calcium carbonate particles are prolifically found in the open sea. Their origin is a mystery but climate change models will need to take them into account.\\nAcid salts and heat give better fish feed\\nSalmon and rainbow trout digest protein in plant-based feed better with inclusion of acid salts in combination with high temperature during extrusion.\\nocean acidification climate change norway forskning.no environment\\n+47 922 47 741 / [email protected]rskning.no', \"A well-established and exclusive year-round village with international schools and a permanent local population, this perennially popular resort sits on a sunny plateau at 1300 metres with magnificent views of Mont Blanc and its massif.\\nVillars celebrated its 150th anniversary as a mountain resort in 2016 and has a loyal following of foreign residents and visitors. It is one of the most easily accessible ski resorts in Switzerland with motorway or rail access direct from Geneva airport to the towns of Aigle and Bex in the valley below. As a home to four international schools, including the famous Aiglon College, there is a healthy demographic mix in the village. It is also one of the best equipped resorts in the Alps with a large sports centre with indoor and outdoor tennis and swimming, a skating rink (the local team was Swiss ice hockey champions in the 1960s) and bowling alley.\\nThe ski area incorporates the terrain above the villages of Villars and Gryon and is linked to Les Diablerets on the other side of the mountain. This creates an area of 44 lifts and 125km of mainly beginner and intermediate slopes, perfect for younger families, as well as glacier skiing up to 3000m. There are three snow parks, 44km of cross country ski trails, toboggan runs and numerous winter walking routes. The thermal baths at Lavey are thirty minutes away and are a popular destination for unwinding after a day's skiing.\\nVillars really comes into its own in the summer months with over 300km of footpaths and 150km of mountain bike trails. There are three mountain lakes above the village with swimming and an excellent restaurant at the furthermost, the Lac des Chavonnes. Golfers can test their skill (and stamina) on the 18 hole golf course which dates from the 1920s. The recently refurbished club house has a good restaurant and magnificent views of Mont Blanc. Lake Geneva is just 30 minutes away with attractions such as the Aquaparc, one of the largest water parks in Europe, as well as famous Riviera towns such as Montreux and Vevey. From here you can explore the fairytale Chateau de Chillon and embark on boat trips to Evian on the French side of the lake.\", 'WILMINGTON, Del., Dec. 10, 2013 (GLOBE NEWSWIRE) -- InterDigital, Inc. (Nasdaq:IDCC), a wireless research and development company, today announced that its Innovation Partners unit has signed a collaboration agreement with VTT Technical Research Centre of Finland, a multi-technological applied research organization in Northern Europe.\\nIn today\\'s hyper-connected world, collection of personal information by merchants, devices, advertisers and technology ecosystem players has become so pervasive and subtle that many users do not even realize that it is happening. Data has become the fuel of the digital gold rush and it sits at the intersection of people, business and technology. New innovations that enhance both privacy and user experience will serve as the foundation for next generation products in the years to come.\\nWith this agreement InterDigital and VTT have partnered with an initial focus on the future of \"user-centric context aware\" research that will create value for the mobile ecosystem. Beyond this first engagement, the parties are working to define further projects to leverage the world class experience and knowledge embodied in the VTT organization.\\nBased in Espoo, Finland, VTT provides high-end technology solutions and innovation services. Innovation Partners recognizes the importance of external research partnerships to capitalize on opportunities to expand the company\\'s research and development efforts in response to the evolution of the mobile industry. Innovation Partners is InterDigital\\'s technology sourcing model based around partnerships with leading technology firms, inventors, universities and research organizations worldwide.\\n\"InterDigital is excited to work with VTT, which has a proven track record of innovation. This announcement aligns with the company\\'s strategy and our commitment to partnerships for external open innovation,\" said Todd Simpson, Executive Vice President of Innovation Partners at InterDigital.\\n\"The cooperation with InterDigital gives to us an opportunity to develop and apply our competencies in a business area with great technology growth potential,\" said Petri Kalliokoski Executive Vice President of VTT.\\nVTT Technical Research Centre of Finland is the biggest multi-technological applied research organization in Northern Europe. VTT provides high-end technology solutions and innovation services.\\nVTT is a part of the Finnish innovation system under the domain of the Ministry of Employment and the Economy. VTT is a not-for-profit organization. VTT has ISO9001:2008 certificate.', 'Since 2008, many private equity and hedge funds have raised capital but placed only part of these monies into deals. Thus, there is a pent up demand to acquire for sound, growing businesses. In addition to this abundance of equity is the low cost of debt financing. These factors are only part of the momentum building for middle market and smaller middle market deals.', 'Q: Can\\'t set custom validator messages in Zend_Form I just can\\'t figure it out how to set custom validator messages in Zend_Form object. Here is an example code.\\n$this->addElement(\\'password\\', \\'password\\', array(\\n        \\'label\\'      => \\'Password\\',\\n        \\'decorators\\' => array(\\'ViewHelper\\'),\\n        \\'filters\\'    => array(\\'StringTrim\\'),\\n        \\'validators\\' => array(\\n            array(\\'Digits\\', false, array(\\'messages\\' => array(\\'notDigits\\' => \\'Only digits are allowed here\\')))\\n        ),\\n        \\'required\\'   => true\\n\\n    ));\\n\\nWhen I try to validate the form entering invalid data  a message saying  \"notDigits\" appear.  I tried to change \\'notDigits\\'  to  Zend_Validate_Digits::NOT_DIGITS, but it still doesn\\'t work as expected.  \\nAny help is greatly appreciated!\\n\\nA: I found my error. I was recieving the \\'notDigits\\' message, because in the controller I used $form->getErrors() method instead of  $form->getMessages().  The first one returns only the error codes, without the messages.\\n\\nA: Your syntax for setting the custom message is correct.  In the code example you posted, the only decorator for that element is ViewHelper so the error message will not be displayed.\\nAt the very least, add the Errors decorator if you want to see the error message.  Try this:\\n$this->addElement(\\'password\\', \\'code\\', array(\\n    \\'label\\'      => \\'Code\\',\\n    \\'decorators\\' => array(\\'ViewHelper\\', \\'Errors\\'),\\n    \\'filters\\'    => array(\\'StringTrim\\'),\\n    \\'validators\\' => array(\\n        array(\\'Digits\\', false,\\n            array(\\'messages\\' => array(\\'notDigits\\' => \\'Only digits are allowed here\\')))\\n    ),\\n    \\'required\\'   => true\\n);\\n\\nThe only change was adding the Errors decorator to the stack.\\n\\nA: Try this validator syntax. \\n$this->addElement(\"text\", \"fullname\", array(\\n                        \\'label\\' => \\'Your Full Name: \\',\\n                        \\'required\\' => \\'true\\',\\n                        \\'validators\\' => array(\\n                            array(\\'validator\\' => \\'StringLength\\', \\'options\\' => array(\\'min\\'=>5, \\'max\\'=>250, \\'messages\\' => array(\\'stringLengthTooShort\\' => \\'The name is too short.\\'))) \\n                        ),\\n                        \\'filters\\' => array(\\'StringTrim\\'),\\n                        \\'decorators\\' => array(\"signup\")\\n                    ));\\n\\n', 'The Louisville Press\\nLake Forest Woman Overreacts to Olympic Closing Ceremony\\nAugust 22, 2016 August 23, 2016 ~ Louisville Press\\nThe Summer Olympics concluded Sunday night with the Closing Ceremony taking place at Maracana Stadium in Rio De Janeiro. Despite low television ratings (down 44% from the Closing Ceremony from London in 2012), one local woman watched intently and enthusiastically.\\nSusan Holt of Lake Forest took to social media following the Closing Ceremony of the summer Olympics in Rio, leaving her family and some of her friends believing that she overreacted.\\nHolt posted to Facebook within minutes of the end of the NBC telecast: \"OMG, that closing ceremony was life-changing! Two words – glorious and devine! The colors, the pageantry, the dancing, the music, the athletes that worked so hard to be there…I\\'ll never take anything for granted again! Even some of the eastern European athletes were smiling! #lifechanging #closingceremony #rioolympics #gloriousanddevine #thecolorsthepageantrythemusictheathletesthatworkedsohardtobethere.\"\\nHolt\\'s children immediately asked her to delete or edit the post, but she refused and had already posted a similar message to Instagram, along with a selfie of herself holding a small Olympic flag.\\n\"I am not changing or deleting any (social media posts)\", said Holt in a phone interview. \"My children can roll their eyes and snicker at me all they like. My mind just soared after watching that ceremony! It was life-affirming, heart-warming, uplifting, and even managed some levity with that Mario Brothers guy doing that thing. As a matter of fact, it was so inspiring that I plan on going to the gym tomorrow\".\\nHolt\\'s reaction took some of her friends by surprise, who called her \"level-headed most of the time\" and \"relatively normal\".\\nNot everyone was surprised by Holt\\'s reaction however, including her husband Bill.\\n\"She overreacts to just about everything\", said Bill. \"Like, at my nephew\\'s wedding last year? I told her I didn\\'t like the (bride\\'s) dress. You\\'d have thought I called (the bride) a hooker or something the way Susan reacted. And just last week, I suggested we go out to eat instead of her making dinner, which of course somehow meant that I hate her cooking. I could go on, but I won\\'t because she\\'ll be reading this article, and of course, overreacting to it\".\\nThe Louisville Press on Facebook\\nThe Louisville Press on Twitter\\n\"And so it goes\" – Kurt Vonnegut\\n< Previous \"Christmas In July\" Sales Fail to Stimulate Local Economy\\nNext > Highly Anticipated Mall St Matthews vs. Oxmoor Center Softball Game to be Played Wednesday', 'Opposition parties could point to the fact that the ANC had an ideal opportunity to remove Zuma and take a stand against corruption but didn\\'t use it, according to Dirk Kotze, a politics professor at the University of South Africa.\\n\"We all have a choice today to do what is just, what is right and what is honorable\", Mmusi Maimane, the leader of the main opposition Democratic Alliance, said in the debate, which Zuma didn\\'t attend.\\nThe lawmakers rejected the no-confidence vote against Zuma on Tuesday, with the ruling African National Congress (ANC) deputies breaking into song and dance to mark the victory of the embattled leader.\\nAlthough a number of analysts didn\\'t foresee that the motion would succeed, markets reacted positively on Monday on the back of Mbete\\'s announcement yesterday that the no confidence vote would be held by secret ballot.\\nVotes will be counted and audited and the secretary of the National Assembly will sign off the results and hand them over to the speaker.\\nTwo groups of demonstrators, one supporting the call for President Zuma to be removed from office and the other against it, have been following Parliamentary proceedings since it began.\\nThe ANC has 249 seats in the National Assembly. \"We are not sellouts, we know who sent us here, we will vote ANC\", she said.\\nWe strongly encourage ANC MP\\'s to search within themselves, put the people of the country first and to do the same. Zuma\\'s supporters were also planning to march. \"I think those MPs must recall that they are the representatives of the people, and must therefore represent the people in terms of what they do this afternoon\", said Mbeki.\\nAlso vying for the leadership is Cyril Ramaphosa, a former trade unionist and one of South Africa\\'s wealthiest politicians.\\nMembers of the opposition EFF (Economic Freedom Fighters) during a march to the Union Buildings, calling for embattled president Zuma to step down, in Pretoria, South Africa.\\nZuma has survived seven no confidence in the past seven years, all conducted by open ballot.\\nMembers of the 400-seat parliament voted 198 to 177 with nine abstentions on Tuesday against the proposal to remove the president.\\n\"Ours is not against the ANC, but against the father of Duduzane, because Duduzane\\'s father is the most corrupt individual in this country\", he said. Another major criticism raised during the parliamentary debate included allegations that he had allowed the state to be \"captured\" by the Guptas, an Indian business family that has been at the centre of a string of media exposés about graft in government and state-owned enterprises.', 'Rear double 1.36 x 1.9 m (4\\'5\" x 6\\'2\"\\nA permanent \\'French\\' bed is complemented with a parallel rear en-suite in the 715, creating a division between the living and the private sleeping area. The kitchen is situated in the centre of the vehicle with the spacious lounge to the front, which when used in conjunction with the swivelling cab chairs can easily accommodation six. A relevant motorhome for those adventuring further on their travels who may want to occasionally entertain or have visitors. A half dinette can be requested with two additional travelling seats and when chosen in Hi-Line the 715 can sleep a total of six adults.\\nCost options available include 147bhp and 177bhp engine upgrades, as well as the Fiat \\'Comfort-Matic\\' sequential transmission.', \"Backgrounder: Regulations for Monitoring Medical Assistance in Dying\\nFrom: Health Canada\\nComing into Force of Regulations for Monitoring Medical Assistance in Dying\\nCanada's federal legislation on medical assistance in dying was enacted on June 17, 2016. Since then, all levels of government have been working together to ensure medical assistance in dying is one of several end-of-life care options available to eligible Canadians.\\nMonitoring is considered to be a critical aspect of virtually all jurisdictions that permit some form of assisted dying. Reporting on the information collected through federal monitoring will support transparency in the application of the law by providing comparable data from across the country. It is expected that this reporting will also foster public trust in how the service is being implemented and delivered across Canada.\\nOn November 1, 2018, regulations supporting a federal pan-Canadian monitoring system on medical assistance in dying will come into force. The regulations require health care providers who are authorized to provide medical assistance in dying to file reports to a designated recipient in their jurisdiction. These reports will:\\nProvide basic information about the individual who has made the request and the practitioner who is reporting;\\nConfirm that eligibility criteria and safeguards were met. If a patient was found ineligible, the reason(s) why will also be reported;\\nInclude the timing and location of the service; and,\\nMeet prescribed deadlines for reporting, which are between 30-120 days depending upon the nature of the provider's involvement in the delivery of the service.\\nThe following health care providers are required to file information for monitoring purposes:\\nPhysicians or nurse practitioners who receive a patient's written request for medical assistance in dying.\\nPharmacists who dispense a substance in connection with the provision of medical assistance in dying.\\nFor many provinces/territories, Health Canada is the designated recipient. Health Canada is developing an online portal jointly with Statistics Canada that that will help make it easy to submit the required information and ensure that the all personal information is protected.\\nSome provinces and territories have a provincial or territorial level designated recipient who will collect the information and forward it to Health Canada. The following chart indicates the designated recipient for each province and territory.\\nProvinces and territories with a federal designated recipient (Health Canada)\\nProvinces and territories with their own designated recipient\\nAlberta (Minister of Health)\\nBritish Columbia (Deputy Minister of Health)\\nNorthwest Territories (Deputy Minister of Health and Social Services)\\nNunavut (Minister of Health)\\nSaskatchewan (Chief Executive Officer of the Saskatchewan Health Authority)\\nOntario (hybrid- provide information to Health Canada in all cases NOT resulting in a MAID death)\\nOntario (hybrid- provide information to the Chief Coroner of Ontario in all cases resulting in a MAID death)\\nThe data will be used to produce annual reports on how the service is being provided across Canada, which will include:\\nStatistics on medical assistance in dying in Canada, including the number of requests, outcomes, medical circumstances and other general characteristics of those requesting and receiving medical assistance in dying;\\nThe application of eligibility criteria and safeguards; and,\\nTrends in medical assistance in dying, as multi-year data become available.\\nNo personal, identifiable information will be publicly released. The data collected will be subject to all applicable federal laws and policies related to the protection of personal information.\\nAdditional information about who must provide information and in what circumstances is available on the Health Canada website.\\nSearch for related information by keyword: Health care | Health care system | Health Canada | Canada | End-of-life care | parents | general public | persons with disabilities | women | youth | media | non-governmental organizations | seniors | government | backgrounders | Hon. Ginette Petitpas Taylor\", 'View of Monterey Bay by Raymond Dabb Yelland\\nHome / MOA Features / View of Monterey Bay by Raymond Dabb Yelland\\nBy MOA Intern\\nIn MOA Features\\nView of Monterey Bay by Raymond Dabb Yelland2019-11-052019-10-31/wp-content/uploads/2012/07/Logo.jpgBYU Museum of Arthttp://moa.byu.edu/wp-content/uploads/image.png200px200px\\nRaymond Dabb Yelland(1848-1900), View of Monterey Bay, 1879, oil on canvas, 53 15/16 x 36 inches. Brigham Young University Museum of Art, gift of S. L. Wright, 1973.\\nGuest post by Kirsten Titus, Marketing & PR Intern\\nRaymond Dabb Yelland was born in London and moved to the United States as a young child. He is known for his American landscape paintings. In his young adulthood, Yelland served in the Union Army during the Civil War. Prior to serving in the military, he attended schooling at the National Academy of Design in New York. One year after graduating, he was hired to be an instructor for the academy. A year later, he worked at the Mills Seminary in California.\\nIn California, he settled with his wife, Annie Meeker. He soon developed a Luminstic style of painting, which consists of a landscape painting that often includes calm imagery of water. This style was inspired by artists such as Sanford Robinson Gifford and John F. Kesset. Yelland continued to be inspired by artists and began to draw inspiration from artists who used Tonalism in their work; these paintings would require artists to paint landscapes using similar color tones throughout their work. Before his death, Yelland taught at distinguished universities in California such as the University of California, Berkeley.\\nYelland painted this work in 1879. During this time, many dreamed of and believed in expanding the United States throughout the American continent, this idea known as \"manifest destiny.\" This painting is a depiction of that nineteenth-century belief. Yelland paints powerful waves and juxtaposes the strong waves with seemingly calm sailboats in the horizon. Symbolic of journey and exploration, the sailboats represent the early Americans pushing from one shore to another and continuing to pursue new land.\\nThis piece is currently on display at the MOA in the Becoming America Exhibition.\\nMOA Intern\\nHappy Halloween Week! – Claude Buck\\'s \"War Protest\"MOA Features\\nLearning from Rembrandt\\'s Religious StyleMOA Features', 'Q: How can one evaluate the following series: $$\\\\sum_{k=1}^{\\\\infty} (-1)^{k}\\\\frac{(\\\\ln{k})^{2}}{k} \\\\space\\\\space\\\\space ?$$\\nFrom $\\\\sum_{n\\\\geq 1}\\\\frac{(-1)^n \\\\ln n}{n}$\\nit has been answered that \\n$$ \\\\sum_{k=1}^{\\\\infty} (-1)^{k} \\\\frac{\\\\ln{k}}{k} = \\\\space \\\\gamma \\\\cdot \\\\ln{2} \\\\space - \\\\space \\\\frac{{(\\\\ln{2})^{2}}}{2} \\\\space \\\\space \\\\space ,  $$\\nwhere $\\\\gamma$ is the Euler-Mascheroni constant.\\n$ \\\\\\\\ $\\n\\nA: Hint. From the standard relation\\n$$\\\\sum_{n\\\\geq 1}\\\\frac{(-1)^n }{n^s}=\\\\left(\\\\frac{1}{2^{s-1}}-1\\\\right)\\\\zeta(s)$$ one may differentiate twice and set $s \\\\to 1^+$ using the Laurent series of the Riemann zeta function.\\n', \"Vernon Williams, MD\\nDr. Williams is a soft spoken, tireless, energetic, caring physician. He has an astute grasp of the big picture. Though he's only 29, he's been an Emergency Medicine Physician for the past 26 years. Dr. Williams considers Urban Wellness & Aesthetics of Houston and Wellness & Aesthetics of San Antonio his life calling.\\nDr. Williams has a BA in Biology from Harvard University. He received his MD from Albert Einstein College of Medicine. He did his General Surgery Residency at Allegheny General Hospital and then became a Flight Surgeon in the US Air Force. Dr. Williams has extensive training in Lasers and Medical Spa procedures. He is a member of the Texas Medical Association (TMA), Fellow of the American Society for Laser Medicine and Surgery (ASLMS), a Diplomat & Fellow of the American Academy of Anti-Aging, Regenerative, & Functional Medicine (A4M), a Fellow of Stem Cells, SportsMedicine, Integrative Cancer Therapies, and Sexual Health with the A4M, a Member of the Anti-Aging Group (AAG) Advisory Board, and a Diplomat of Age Management Medicine of Cenegenics Medical Institute.\", 'Cosmic cannibalism seen in action\\nA simulation of Triangulum\\'s orbit around Andromeda. John Dubinski and Larry Widrow\\nStar density map around the Andromeda and Triangulum Galaxies. John Dubinski and Larry Widrow\\nPress release issued: 4 September 2009\\nStars and giant structures around the Andromeda Galaxy – many seen for the first time – are probably the remnants of smaller galaxies that have been cannibalised by Andromeda, finds a new study published online today in Nature.\\nStars and giant structures around the Andromeda Galaxy – many seen for the first time – are probably the remnants of smaller galaxies that have been cannibalised by Andromeda, finds a new study published online today in N ature.\\nThe new images, captured by an international team of astronomers including the University of Bristol, are from the largest panoramic survey of Andromeda ever made. The survey covers a region nearly a million light years across.\\nIt charted Andromeda\\'s unexplored outskirts for the first time and detected faint stars and giant structures that are almost certainly remnants of smaller galaxies, cannibalised by Andromeda as part of its ongoing expansion.\\nAt 2.5 million light years from our own galaxy – the Milky Way Galaxy – the Andromeda Galaxy (M31) is our closest giant neighbour. It is so large it is visible to the naked eye from the Northern Hemisphere.\\nAnother nearby galaxy, the Triangulum Galaxy (M33), was seen to be surrounded by a giant stellar \\'halo\\' that the team believes provides persuasive evidence for its recent encounter with Andromeda. Stars are pulled from Triangulum as it orbits Andromeda due to the strong gravity of this massive galaxy. These stars are then consumed by Andromeda and contribute to its unstoppable expansion.\\nDr Avon Huxor, from the University of Bristol\\'s Physics Department and an author on the paper, said: \"Galaxies are large collections of stars and other matter that are held together by gravity. Theory holds that they evolve and grow by absorbing smaller galaxies over time. One way to test this idea is to find the left-overs from this process.\\n\"But finding these faint structures is difficult, since it involves looking over an area hundreds of times larger than the main disk of the galaxy. The really exciting thing about this study is that we can see this merging of galaxies in real detail for the first time. Oddly enough, it is hard to do this with our own Galaxy, as we sit within it, which makes it difficult to disentangle what the data is telling us.\"\\nDr. Alan McConnachie, from the NRC Herzberg Institute of Astrophysics in Canada, who led the survey team, said: \"That we are finding streams and structures over the entire survey area is a startling demonstration of the vast size of this \\'typical\\' galaxy. Until a few years ago, no-one expected to find anything this far from the centre of a galaxy. But Andromeda is showing us that galaxies are much bigger than we originally thought.\"\\nThe results published today set the stage for a more detailed reconstruction of the formation of Andromeda, a process that appears to be continuing to this day.\\nThe team includes astronomers from the National Research Council Canada (NRC) working alongside university-based researchers from across Canada, as well as colleagues in Australia, France, Germany, the UK and the USA.\\nThe paper: Revealing the history of tidal interactions in the Andromeda and Triangulum galaxies, by Alan W. McConnachie, Michael J. Irwin, Rodrigo A. Ibata, John Dubinski,Lawrence M. Widrow, Nicolas F. Martin, Patrick Cote, Aaron L. Dotter, Julio F. Navarro, Annette M. N. Ferguson, Thomas H. Puzia, Geraint F. Lewis, Arif Babul, Pauline Barmby, Olivier Bienayme, Scott C. Chapman, Robert Cockcroft, Michelle L. M. Collins, Mark A. Fardall, William E. Harris, Avon Huxor, A. Dougal Mackey, Jorge Penarrubia, R. Michael Rich, Harvey B. Richer, Arnaud Siebert, Nial Tanvir, David Valls-Gabaud, Kimberly A. Venn. Nature, 3 September 2009.\\nPlease contact Cherry Lewis for further information.\\nStudent notices\\nAll news feeds', 'Lutefisk Sushi Volume E was just reviewed at Stumptown Trade Review here.\\nOverall the entire Lutefisk Sushi project is an interesting experiment which rewards the reader with a wide variety of comics, many of which would likely escape notice if they were just presented on their own on a table at a con. Other comics collectives should take note of what Cartoonist Conspiracy has done here. They have created a fantastic product which gives all of its members a showcase, comes in a pleasing package, and makes the reader not only take notice of individual comic creators, but also creates a single product which invites repeat purchases. I, for one will be back for more helpings of Lutefisk Sushi!\\nGo read the full review at the Stumptown Trade Review site.', 'Ending world hunger by focusing on women and education: Prove sponsors The Hunger Project\\nMove the Chain\\nAs of July 2022, approximately 828 million people are facing hunger, according to the World Food Programme – and that 828 million isn\\'t the largest figure for world hunger. The FAO estimates that there are 2.3 billion people, roughly 29% of the global population, facing less extreme, but still dangerous, levels of food insecurity. Most hungry people are living in chronic hunger, which often happens over many generations.\\nLuckily, hunger can be avoided. While these numbers sound alarming and a solution may seem far-fetched, we know that there is more than enough food produced in the world to feed everyone on the planet. So why is this happening?\\nThe reason is simple: Handouts and distributing food are rarely the solution. These methods have proven time and time again to be unsustainable. You may have heard the proverb: \"If you give a man a fish, you feed him for a day. If you teach a man to fish, you feed him for a lifetime.\" Many Western aid organizations have been shifting away from giving charity in developing countries to promoting self-reliance in sustainable programs. While there are different schools of thought on the efficacy of this method, it seems that some nonprofits have identified the key to success using this methodology.\\nOne of these nonprofits is The Hunger Project, a global organization trying to solve world hunger by investing in people\\'s capacity, creativity and leadership. Rather than making small contributions at the periphery, The Hunger Project works at the heart of the problem – building self-reliance at the grassroots level, empowering women as key change agents, and forging effective partnerships with local governments. Mindset shift is at the heart of The Hunger Project\\'s programs. Through their \"Vision, Commitment, and Action\" workshops, mentoring and a tailored approach, The Hunger Project helps communities envision a future of their own making.\\nWhy is there a focus on women and education? According to the Hunger Project\\'s research, women in Africa farm more than men do, but they are more likely to go hungry. The Hunger Project has helped decrease severe hunger by more than 42%, household poverty by 30% and increase female business ownership by 31% (Source: The Hunger Project, Household Evaluation Data, 2015 - 2021)\\nProve supports the cause by matching your donations\\nWe\\'re excited to announce that Prove, a pioneer in identity verification, will be sponsoring The Hunger Project by raising funds during the holidays. Through the fundraiser, appropriately named \\'More Than A Meal\\', Prove hopes to raise $10,000 by December 31, 2022 via Move the Chain platform.\\nIf you are hoping to give a gift that helps make a difference for people facing hunger on Giving Tuesday or during the holidays, here is your opportunity. Prove is going to match your donations: every dollar donated to the Hunger Project through the fundraiser, Prove will match (up to $10,000). You can also support the cause by simply sharing the fundraiser with family and friends, and on social media. Thank you for making a difference during the holidays!\\nHunger and food scarcity are on the rise in NYC. SitusAMC is supporting Food1st to make an impact.\\nIn New York City, over 30% of our trash is made up of food waste. At the same time, New Yorkers are facing food insecurity. Every day, food sits in landfills while our neighbors go hungry. Hunger doesn\\'t affect everyone equally in New York City. Low-income communities are especially affected\\nSitusAMC supports Hurricane relief efforts\\nHurricane Ian made landfall in Florida on September 28, 2022, devastating large areas of the state. Six weeks later, hundreds of people were still in shelters, and many more depended on help for food, water, and other support. Various organizations have been working tirelessly to help people surmount the heartbreaking\\nDestigmatize Mental Health within the AAPI Community\\nMental health has become central to an increasing number of companies, influencers, and nonprofits, as their own experience with mental issues carved a path for a better future. The month of May recognizes Asian American and Pacific Islander Heritage Month. Leveraging the Move the Chain platform, Prove, an authentication company,\\nMove the Chain © 2023', \"Can it really be 40 years since Monty Python's Life of Brian was released to a controversy that has had few parallels since?\\nWatching it again after some time and from this distance it looks more than ever as a natural successor to The Holy Grail; as a collection of sketches pinned together with a slim narrative. That is still the case but it has a more fluid continuity than its predecessor and doesn't jar quite so much, which is the main problem for me with Holy Grail.\\nLovingly presented in its original standard definition it's threadbare production probably wouldn't benefit too much from a buff up in all honesty. But that's not what this is about as the essential element is the script which apart from some bits has aged very well. You can stop the film anywhere you want and you'll find a bone-fide classic sequence. And as well repeated as they are, they are still very funny.\\nNever the greatest of actors the team just grab their allotted character(s) and run with them: Big Nose on the mount is still base raw comedy of insults, the sophistication of Brian's grammatically poor Latin graffiti and the teasing of Pontius Pilate in the throne room as the guards' corpseing are just some. There are many and it is infinitely quotable.\\nWhat is interesting to consider now is that the narrative thread of Brian (Graham Chapman) doesn't these days look that offensive and tends to cast doubt on the oft cited assertion that this film couldn't be made today. Granted it does have a clear reference to the life of Jesus but I would suggest, it is actually quite respectful. Brian is by and large an innocent caught up in circumstances and it certainly doesn't appear blasphemous.\\nThe film however is scathing of those around him and what they represent: The pseudo intellectuals of the various Judean factions who debate endlessly and meaninglessly, and like many movements end up splitting and fighting amongst themselves. And, of course there's peoples' gullibility and weaknesses as they search for direction and guidance.\\nWhat could play more difficult with a first-time audience (and does stick out more now than it did on first release) is the perceived mocking of some conditions such as stammering and maybe mental illness. If these scenes are awkward to watch now then they have to be seen in the context of the time it was made, and people can watch safe in the knowledge that it is very unlikely that these conditions would be held up to ridicule today.\\nMonty Python's Life of Brian has a limited one-day release at selected cinemas on 18 April.\", 'A series of half-hour documentaries that examine outstanding people and communities in Ontario. Hosted by Joan Reed-Olsen, \"People Patterns\" looks at what makes these people and communities special. The series also deals with social issues affecting the province.\\nRaccoons, skunks, wounded deer, and orphaned beavers find their way to the Aspen Valley Wildlife Sanctuary.', 'Remembering Ann Richards\\nAUSTIN, Texas—She was so generous with her responses to other people. If you told Ann Richards something really funny, she wouldn\\'t just smile or laugh, she would stop and break up completely. She taught us all so much—she was a great campfire cook. Her wit was a constant delight. One night on the river on a canoe trip, while we all listened to the next rapid, which sounded like certain death, Ann drawled, \"It sounds like every whore in El Paso just flushed her john.\"\\nShe knew how to deal with teenage egos: Instead of pointing out to a kid who was pouring charcoal lighter on a live fire that he was idiot, Ann said, \"Honey, if you keep doing that, the fire is going to climb right back up to that can in your hand and explode and give you horrible injuries, and it will just ruin my entire weekend.\"\\nShe knew what it was like to have four young children and to be so tired you cried while folding the laundry. She knew and valued Wise Women like Virginia Whitten and Helen Hadley.\\nAt a long-ago political do at Scholz Garten in Austin, everybody who was anybody was there meetin\\' and greetin\\' at a furious pace. A group of us got the tired feet and went to lean our butts against a table at the back wall of the bar. Perched like birds in a row were Bob Bullock, then state comptroller, moi, Charles Miles, the head of Bullock\\'s personnel department, and Ms. Ann Richards. Bullock, 20 years in Texas politics, knew every sorry, no good sumbitch in the entire state. Some old racist judge from East Texas came up to him, \\'Bob, my boy, how are you?\"\\nBullock said, \"Judge, I\\'d like you to meet my friends: This is Molly Ivins with the Texas Observer.\"\\nThe judge peered up at me and said, \"How yew, little lady?\"\\nBullock, \"And this is Charles Miles, the head of my personnel department.\" Miles, who is black, stuck out his hand, and the judge got an expression on his face as though he had just stepped into a fresh cowpie. He reached out and touched Charlie\\'s palm with one finger, while turning eagerly to the pretty, blonde, blue-eyed Ann Richards. \"And who is this lovely lady?\"\\nAnn beamed and replied, \"I am Mrs. Miles.\"\\nOne of the most moving memories I have of Ann is her sitting in a circle with a group of prisoners. Ann and Bullock had started a rehab program in prisons, the single most effective thing that can be done to cut recidivism (George W. Bush later destroyed the program). The governor of Texas looked at the cons and said, \"My name is Ann, and I am an alcoholic.\"\\nShe devoted untold hours to helping other alcoholics, and anyone who ever heard her speak at an AA convention knows how close laughter and tears can be.\\nI have known two politicians who completely reformed the bureaucracies they were elected to head. Bob Bullock did it by kicking ass at the comptroller\\'s until hell wouldn\\'t have it. Fear was his m.o. Ann Richards did it by working hard to gain the trust of the employees and then listening to what they told her. No one knows what\\'s wrong with a bureaucracy better than the bureaucrats who work in it.\\nThe 1990 race for governor was one of the craziest I ever saw, with Ann representing \"New Texas.\"\\nRepublican nominee Claytie Williams was a perfect foil, down to his boots, making comments that could be construed as racist and sexist. Ann was the candidate of everybody else, especially for women. She represented all of us who have lived with and learned to handle good ol\\' boys, and she did it with laughter. The spirit of the crowd that set off from the Congress Avenue Bridge up to the Capitol the day of Ann\\'s inauguration was so full of spirit and joy. I remember watching San Antonio Mayor Henry Cisneros that day with tears running down his cheeks because Chicanos were finally included.\\nAnn got handed a stinking mess: Damn near every state function was under court order. The prisons were so crowded, dangerous convicts were being let loose. She had a long, grinding four years and wound up fixing all of it. She always said you could get a lot done in politics if you didn\\'t need to take credit.\\nBut she disappointed many of her fans because she was so busy fixing what was broken, she never got to change much. The \\'94 election was a God, gays and guns deal. Annie had told the legislature that if they passed a right-to-carry law, she would veto it. They did, and she did. At the last minute, the NRA launched a big campaign to convince the governor that we Texas women would feel ever so much safer if we could just carry guns in our purses.\\nSaid Annie, \"Well, you know that I am not a sexist, but there is not a woman in this state who could find a gun in her handbag.\"\\n© Copyright 2006 Creators Syndicate\\nMolly Ivins (August 30, 1944 – January 31, 2007) was an American newspaper columnist, liberal political commentator, humorist and author. From Americans Who Tell the Truth: \"To honor a journalist as a truth teller is implicitly to comment on the scarcity of courage and candor in a profession ostensibly dedicated to writing and speaking the truth. Molly Ivins is singular in her profession not only for her willingness to speak truth to power but for her use of humor to lampoon the self-seeking, the corrupt and the incompetent in positions of public trust. Her wit and insight place her squarely in the tradition of America\\'s great political humorists like Mark Twain.\"\\nHas Nothing Been Learned Since 2003? Corporate Media Welcome Back Iraq War Hawks To Make Case for Iran\\nWe Need More Than the Relatively Useless War Powers Resolutions\\n\\'Utterly Shameful\\': Progressives Slam Texas GOP Gov. Greg Abbott\\'s Decision to Refuse New Refugees\\nCheers as Federal Judge Blocks Trump Order Allowing Localities to Refuse New Refugees\\nMolly Ivins, George W. Bush, Texas', 'Are the Democrats sunsetting Joe Biden?\\nThere are forces in the Democratic establishment who would like him to take a long-overdue retirement\\nFreddy Gray\\nBy Freddy Gray\\nhttps://thespectator.com/topic/democrats-sunsetting-joe-biden-documents/\\nHas Joe Biden suddenly outlived his usefulness? That is what many conspiracy-theory-inclined Americans are saying as the fuss mounts around the classified documents found at one of the president\\'s offices and his home in Delaware.\\nThe theory goes something like this: the midterms are out of the way, Trump\\'s power is waning, and everybody in Washington knows that Joe Biden is too old and doddery to fight on to the 2024 presidential election and beyond. So, the \"Deep State\" — the secretive government powers who really rule America — feels it can now safely shuffle out...\\nThe theory goes something like this: the midterms are out of the way, Trump\\'s power is waning, and everybody in Washington knows that Joe Biden is too old and doddery to fight on to the 2024 presidential election and beyond. So, the \"Deep State\" — the secretive government powers who really rule America — feels it can now safely shuffle out the old man and bring in another Democratic leader. Dial up the sinister background music.\\nAt one level, such thinking is yet another example of the paranoid style in American politics. Yet conspiracy theories often derive their power from elements of truth.\\nBiden is the war president Ukraine needs\\nOh no: Adam Schiff announces for California Senate\\nJoe Biden\\'s classified doc shocker\\nHalfway through Harris: our remarkable VP\\nWhy Biden\\'s document scandal is worse than Trump\\'s\\nThere are forces in the Democratic establishment who would now like Joe Biden to take his long-overdue retirement. And the circumstances of this \"classified documents\" story are fishy, to put it mildly. Just because you\\'re paranoid, as Joseph Heller didn\\'t quite say, doesn\\'t mean they\\'re not lying to you.\\nLet\\'s start with the timing. The first ten of these documents were found, allegedly, on November 2, just before the midterm elections. On that day, Joe Biden\\'s personal lawyers just happened to be rooting around his private office at the Penn Biden Center for Diplomacy and Global Engagement when they came across a file marked \"personal\" that contained some sensitive state documents that shouldn\\'t have been there. As every loyal pro-Democratic hack in America will be quick to add, they then did the right thing, alerted the National Archives and promptly handed them over. How very much more proper than dastardly Donald Trump, who illegally hoarded a far larger number of documents at Mar-a-Lago and refused to give them back etc, etc.\\nBut was that initial discovery as accidental as Team Biden makes out? What were Biden\\'s private lawyers doing digging about in that office at that time? Could it have something to do with the fact the Republicans were just about to win back the House of Representatives, which would give Biden\\'s opponents the congressional power to investigate parts of his past that he would rather leave unexplored?\\nDonald Trump calls the Biden family the \"the Biden crime family\" — and that, of course, is Trumpian hyperbole. But anybody who has read in any detail about the business affairs of Jim Biden, Joe\\'s brother, and the president\\'s notoriously troubled son Hunter, knows that the slur is not altogether inaccurate. There are plenty of dodgy dealings in the Bidens\\' past. The shadiest period appears to have been during Barack Obama\\'s second term, when Joe was still vice president and the family endeavored to cash in on his influence wherever possible.\\nWhich brings us back to these discovered files with classified markings, which all allegedly stem from Biden\\'s time as VP. And some other important questions: what exactly is the purpose of the Penn Biden Center for Diplomacy and Global Engagement of the University of Pennsylvania, which was established in 2017, after Joe Biden left office? Is it just, as its website says, to \"engage Penn\\'s students and partners with its faculty and global centers to convene world leaders, develop and advance smart policy.\"\\nIn the two years after it was launched, foreign donations to the University of Pennsylvania tripled, reaching $61 million — much of it from China. How involved was former vice president Joe Biden, who at that time was making considerable sums of money himself, in the Center that bore his name? Clearly, classified government files from his time in power found their way to the Center\\'s Washington office.\\nAnother key question: why is the story only coming out now — two months after the first lot of files were handed over and almost a month after Biden\\'s lawyers flagged that another set of \"files with classified markings\" had been discovered in Biden\\'s home in Delaware?\\nThe story has broken just after Joe Biden may or may not have made a decision about his future during his Christmas holiday in the Caribbean. After a rough couple of years, Biden appeared to be enjoying what one of his aides called a \"jolt\" of political momentum as 2023 began. The word in Washington was that, buoyed by recent developments and the hopes of an improving American economy, he was going to announce his intention to run again. I wrote a cover piece for our UK edition about it last week.\\nNow, suddenly, he\\'s fumbling awkward replies about a possible scandal — one that is particularly embarrassing for Biden given the piety with which his party lectured Donald Trump about the sanctity of state secrecy.\\nThe parallels between the \"Trump hoarding files\" story and the \"Biden hoarding files\" story could break both ways, too. On the one hand, Biden\\'s defenders will say his sloppiness was nothing, in scale and intent, compared to Trump\\'s flagrant flouting of the rules about classified material, and they may well be right.\\nOn the other hand, the comparison could end up offering Biden a neat way of stepping down with his pride intact. \"Unlike my predecessor I take national security very seriously,\" he could say, or words to that effect. \"If I have been found to have failed in my duties in any respect…\" You get the picture.\\nThe media might then spend a few news cycles describing him as an honorable man who fell on his sword, the political disaster that is Kamala Harris would fill the breach, and the contest for 2024 would be thrown open again. That\\'s all highly speculative, of course. But then we live in a time when Joe Biden is president and very strange things are happening in the heart of American power.\\nThis article was originally published on The Spectator\\'s UK website.\\nby Freddy Gray Freddy Gray is deputy editor of The Spectator and the founding editor of The Spectator\\'s World edition. He was formerly literary editor of the American Conservative.\\nFace it: Donald Trump isn\\'t going anywhere\\nHarry and Meghan\\'s authoritarian streak\\nFreddy Gray is deputy editor of The Spectator and the founding editor of The Spectator\\'s World edition. He was formerly literary editor of the American Conservative.\\nThirty years from Waco: what the fatal siege wrought\\nBy Kevin Cook\\nIn defense of paranoia\\nBy Bridget Phetasy\\nStepping out into freedom\\nBy Roger Kimball\\nThe media turns on Joe Biden — for the moment\\nBy Ben Domenech\\nSantos\\'s little helpers\\nControversial Energy Department official quietly exits\\nBy Matthew Foldi', \"The list below includes the cities that the US Post Office accepts for ZIP code 02637. The preferred city may not be the city in which the ZIP is located. The city for 02637 is usually the name of the main post office. When mailing your package or letter, always include the preferred or acceptable cities. Using any city in the list of unacceptable cities may result in delays.\\nZIP code 02637 is located in southeast Massachusetts and covers a slightly less than average land area compared to other ZIP codes in the United States. It also has a slightly less than average population density.\\nThe people living in ZIP code 02637 are primarily white. The number of seniors is extremely large while the number of middle aged adults is extremely large. There are also a large number of families and a small number of single parents. The percentage of children under 18 living in the 02637 ZIP code is extremely small compared to other areas of the country.\\nZIP code 02637 has a large percentage of vacancies.\\nThe majority of household are owned or have a mortgage. Homes in ZIP code 02637 were primarily built in the 1970s. Rentals in 02637 are most commonly 2 bedrooms. The rent for 2 bedrooms is normally $750-$999/month including utilities. Prices for rental property include ZIP code 02637 apartments, townhouses, and homes that are primary residences.\\nFor more information, see Barnstable Town, MA home prices.\\nAs with most parts of the country, vehicles are the most common form of transportation to places of employment. Pedestrians and cyclists beware. The area has some of lowest percentages of commutes without a vehicle in the country. Compared to other ZIP codes in the country, 02637 has very few people that work at home. In most parts of the country, the majority of commuters get to work in under half an hour. The commute in 02637 is that short for a higher percentage of workers than almost anywhere in the United States. Many commuters should consider themselves lucky that they don't have a longer commute. It is very uncommon, compared to the rest of the US, for employees to have to travel more than 45 minutes to their place of employment.\", 'Spangler Candy has sent CandyStore.com a statement – spelled out in candy hearts – after the outpouring of affection and disappointment at SweetHearts absence this year.\\nView the latest update on the SweetHearts.\\nCandystore.com has reached back out to Spangler for comment on the likelihood that SweetHearts will return in 2020 but have yet to receive a response.', 'Why Did The Civil War Last So Long? (11 Reasons Why)\\nTriggered by different stances on slavery, the American Civil War lasted 1487 days. But did you know that the Union had expected their war with the Confederate States to last no longer than 90 days?\\nThis brings us to today\\'s question: why did the Civil War last so long? Well, here are 11 reasons that contributed to the time stretch.\\nWhy Did The Civil War Last So Long?\\n1. Generals Of The North Were Cautious\\nOne of the main reasons the Civil War in America lasted more than 4 years is that the North generals were too timid at first. They lacked the fierceness that commanders should have in such situations.\\nPerhaps the most notoriously cautious leading figure on the Union\\'s side was General George B. McClellan.\\nRepeatedly, he failed to take the initiative when he had an advantage over the South or capitalize on any success that the Union had secured.\\nThis resulted in President Lincoln expressing his frustration in a letter addressed to General McClellan in October 1862.\\nAbout a month later, President Lincoln replaced General McClellan with General Ulysses S. Grant who was much more assertive.\\n2. Clever Leaders Of The South\\nOn the other hand, the Confederacy had highly skilled and capable commanders right off the bat. This was one of the greatest advantages when it came to leading the South army.\\nTopping the list is General Robert Edward Lee.\\nHe was the commander of the Confederacy\\'s army from the beginning until its surrender, including at its most powerful faction; the Army of Northern Virginia.\\nOther significant names include Generals J. E. B. Stuart, Stonewall Jackson, James Longstreet, Patrick Cleburne, and Nathan Bedford Forrest.\\n3. Union\\'s Side Had A Bad Start\\nA good start is half the job done. However, the North clearly didn\\'t get the memo as they had one of the worst possible attitudes toward the conflict between their government and the Confederacy.\\nFor some reason, the Union\\'s Federal government chose to ignore the issue as if that\\'s going to make it go away. This gave the South the perfect chance to establish an army and fortify their defenses without the enemy breathing down their necks.\\nRead More: Why Did The Vietnam War Last So Long? (11 Reasons Why)\\nBy the time the Federal government had noticed what was happening, the Confederacy had taken leaps ahead.\\nBefore engaging in any major battles, the North had lost tens of thousands of soldiers and many important officers. They also lost heaps of weapons and various forts, which put them at a serious disadvantage.\\nOne could argue that slow and steady wins the race, but the North really dropped the ball here. What they could\\'ve wrapped up in a few months took too many years and lives to finish.\\n4. Defense Was The South\\'s Strategy\\nBeing on the defense was the strategy adopted by the Confederacy from the very beginning. They knew their resources weren\\'t as vast as the North, so they had to be smart about their approach.\\nLooking back now, we can say that it worked for an extended period. The South had one main goal to focus on, which is sustaining its existence as a separate political entity for as long as possible.\\nSticking to such a completely defensive strategy also gave the Confederacy the advantage of being the home side.\\nFighting on their ground allowed the South army to be in a position of tactical superiority, even with much fewer numbers and less experience compared to the North military.\\nNot to mention, the strategy of the Union was to dominate the Confederacy to obtain victory. This meant that the North had to break the defenses of the South to win, which took a long time.\\n5. North\\'s Military Had To Retake Territory\\nEvery action in war has consequences. This is evident in the sequence of events comprising the American Civil War, particularly in this reason for its long duration.\\nFollowing the bad start provided by the Union and the resulting loss of soldiers, weapons, and forts, they had to do some damage control with a series of tasks to shift some of the power back to their favor.\\nRead More: Why Was Princess Diana So Loved? (11 Reasons Why)\\nOne of these tasks was to retake territory of around 750,000 square miles.\\nThese lands weren\\'t an easy target either. They were heavily defended by thousands of soldiers who knew the different routes and terrain like the back of their hand.\\nOf course, this hindered the advancement of the North and extended the war period.\\n6. Decision-Makers Didn\\'t Want To Be Wasteful\\nOnce resources have been spent and blood has been shed, most leaders will dismiss the idea of stopping war to prevent all these sacrifices don\\'t go to waste.\\nDecision-makers in the Civil War weren\\'t necessarily different, be it on the Union\\'s or the Confederacy\\'s side. Both sides had invested too much to back down.\\nIt goes without saying that this approach is faulty. Knowing when to cut your losses is a skill on its own, and war should only continue if the outcome is better and comes at a reasonable cost.\\nSadly, politicians and leaders may not have the same views, especially if the conflict is domestic.\\n7. They Had Winter Breaks\\nDuring times of war, it\\'s no secret that when winter rolls in, fighting becomes almost impossible (unless you\\'re Russia).\\nFor this reason, it\\'s common practice that both sides of war seize fighting during winter. The American Civil War was no exception.\\nArmies of the North and the South took several months off over the years to winter, during which there was no actual fighting happening and, consequently, no progress.\\n8. Communication Was Poor\\nIn the 1860s, communication methods were nothing like they are now. While you can deliver and receive information in mere seconds today, back then it took days or even weeks on end.\\nTo help put things in better perspective, the telegraph was a pretty new technology at the time.\\nWith such delayed means of communication, it took a very long time by today\\'s standards to send instructions and receive orders.\\nThis goes for relaying any type of information from one state to the other. The duration was extended even more due to the cautionary measures that messengers had to take to avoid being intercepted by the enemy.\\nRead More: Why Is Jack The Ripper Famous? (11 Reasons Why)\\n9. Supply Chains Were Underdeveloped\\nSimilar to the poor communication methods of the time, supply chains were also very undeveloped compared to after the Civil War.\\nThe railway system was far from completion, the logistical experience was severely lacking at best, and the infrastructure was still in its crib.\\n10. Soldiers Were Few And Untrained\\nOne additional reason that the Civil War lasted so long was that there weren\\'t really enough soldiers on both sides to get the job done. Even the numbers that existed were mostly untrained.\\nIt took time for the North and the South to build their military forces. Not to mention, the Union lost plenty of its power early on and had to re-establish it.\\n11. Crossing The Appalachian Range Was Tough\\nLast but not least, the mountains of the Appalachian range residing near the east coast played a part. They geographically slowed down the march of soldiers.\\nGranted, these mountains aren\\'t especially tall, but they were slippery and dangerous.\\nCrossing them was almost a suicide mission due to falls and getting stuck in narrow routes where troops could easily be cornered into an ambush.\\nTo learn more, you can also read our posts on why this world is so messed up, why men wore white wigs, and why humans are so weak.\\nAs you can tell by now, there are various reasons to answer the question \"why did the Civil War last so long?\"\\nFrom skillful commanders, timid generals, and defensive strategies to poor communication, harsh winters, and underdeveloped supply chains, the Civil War had it all.\\nCategories History\\nWhy Are Kangaroos So Buff? (9 Reasons Why)\\nWhy Do Guys Wear Their Pants Sagging? (7 Reasons Why)'], 'meta': {'redpajama_set_name': ['RedPajamaCommonCrawl', 'RedPajamaCommonCrawl', 'RedPajamaCommonCrawl', 'RedPajamaWikipedia', 'RedPajamaC4', 'RedPajamaCommonCrawl', 'RedPajamaCommonCrawl', 'RedPajamaC4', 'RedPajamaC4', 'RedPajamaC4', 'RedPajamaStackExchange', 'RedPajamaCommonCrawl', 'RedPajamaC4', 'RedPajamaC4', 'RedPajamaCommonCrawl', 'RedPajamaCommonCrawl', 'RedPajamaStackExchange', 'RedPajamaCommonCrawl', 'RedPajamaCommonCrawl', 'RedPajamaC4', 'RedPajamaCommonCrawl', 'RedPajamaC4', 'RedPajamaC4', 'RedPajamaCommonCrawl', 'RedPajamaCommonCrawl', 'RedPajamaC4', 'RedPajamaC4', 'RedPajamaCommonCrawl']}}\n"
     ]
    }
   ],
   "source": [
    "dataloader = DataLoader(dataset, batch_size=32)\n",
    "print(type(dataloader))\n",
    "flag = True\n",
    "for batch in dataloader:\n",
    "    # 处理你的数据\n",
    "    if(flag):\n",
    "        #print(batch)\n",
    "        print(type(batch))\n",
    "#print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Incorrect path_or_model_id: '/workspace/mistral-7B-PoSE-32k'. Please provide either the path to a local folder or the repo_id of a model on the Hub.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHFValidationError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/utils/hub.py:402\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    400\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    401\u001b[0m     \u001b[38;5;66;03m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[0;32m--> 402\u001b[0m     resolved_file \u001b[38;5;241m=\u001b[39m \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    403\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    404\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    405\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    406\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    407\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    408\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    409\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    410\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    411\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    412\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    413\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    414\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    415\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    416\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m GatedRepoError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_deprecation.py:101\u001b[0m, in \u001b[0;36m_deprecate_arguments.<locals>._inner_deprecate_positional_args.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    100\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m)\n\u001b[0;32m--> 101\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_validators.py:106\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m arg_name \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrepo_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrom_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto_id\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m--> 106\u001b[0m     \u001b[43mvalidate_repo_id\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg_value\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m arg_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m arg_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_validators.py:154\u001b[0m, in \u001b[0;36mvalidate_repo_id\u001b[0;34m(repo_id)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m repo_id\u001b[38;5;241m.\u001b[39mcount(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 154\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HFValidationError(\n\u001b[1;32m    155\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRepo id must be in the form \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrepo_name\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m or \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnamespace/repo_name\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    156\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrepo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. Use `repo_type` argument if needed.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    157\u001b[0m     )\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m REPO_ID_REGEX\u001b[38;5;241m.\u001b[39mmatch(repo_id):\n",
      "\u001b[0;31mHFValidationError\u001b[0m: Repo id must be in the form 'repo_name' or 'namespace/repo_name': '/workspace/mistral-7B-PoSE-32k'. Use `repo_type` argument if needed.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[270], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoTokenizer\n\u001b[1;32m      3\u001b[0m context_length \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m32000\u001b[39m\n\u001b[0;32m----> 4\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mAutoTokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/workspace/mistral-7B-PoSE-32k\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mtrust_remote_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m outputs \u001b[38;5;241m=\u001b[39m tokenizer(\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;66;03m#raw_datasets[\"train\"][:2][\"content\u001b[39;00m\n\u001b[1;32m      8\u001b[0m     batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     12\u001b[0m     return_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     13\u001b[0m )\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput IDs length: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(outputs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/auto/tokenization_auto.py:834\u001b[0m, in \u001b[0;36mAutoTokenizer.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    831\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer_class\u001b[38;5;241m.\u001b[39mfrom_pretrained(pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    833\u001b[0m \u001b[38;5;66;03m# Next, let's try to use the tokenizer_config file to get the tokenizer class.\u001b[39;00m\n\u001b[0;32m--> 834\u001b[0m tokenizer_config \u001b[38;5;241m=\u001b[39m \u001b[43mget_tokenizer_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    835\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m tokenizer_config:\n\u001b[1;32m    836\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m tokenizer_config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/auto/tokenization_auto.py:666\u001b[0m, in \u001b[0;36mget_tokenizer_config\u001b[0;34m(pretrained_model_name_or_path, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, **kwargs)\u001b[0m\n\u001b[1;32m    663\u001b[0m     token \u001b[38;5;241m=\u001b[39m use_auth_token\n\u001b[1;32m    665\u001b[0m commit_hash \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m--> 666\u001b[0m resolved_config_file \u001b[38;5;241m=\u001b[39m \u001b[43mcached_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    667\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    668\u001b[0m \u001b[43m    \u001b[49m\u001b[43mTOKENIZER_CONFIG_FILE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    669\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    670\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    671\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    672\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    673\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    674\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    675\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    676\u001b[0m \u001b[43m    \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    677\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_raise_exceptions_for_gated_repo\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    678\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_raise_exceptions_for_missing_entries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    679\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_raise_exceptions_for_connection_errors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    680\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    681\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    682\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m resolved_config_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    683\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not locate the tokenizer configuration file, will try to use the model config instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/utils/hub.py:466\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    464\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThere was a specific connection error when trying to load \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00merr\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    465\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HFValidationError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 466\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    467\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIncorrect path_or_model_id: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. Please provide either the path to a local folder or the repo_id of a model on the Hub.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    468\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    469\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resolved_file\n",
      "\u001b[0;31mOSError\u001b[0m: Incorrect path_or_model_id: '/workspace/mistral-7B-PoSE-32k'. Please provide either the path to a local folder or the repo_id of a model on the Hub."
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "context_length = 32000\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"/workspace/mistral-7B-PoSE-32k\",trust_remote_host=True)\n",
    "\n",
    "outputs = tokenizer(\n",
    "    #raw_datasets[\"train\"][:2][\"content\n",
    "    batch['text'][0],\n",
    "    truncation=True,\n",
    "    max_length=context_length,\n",
    "    return_overflowing_tokens=True,\n",
    "    return_length=True,\n",
    ")\n",
    "\n",
    "print(f\"Input IDs length: {len(outputs['input_ids'])}\")\n",
    "print(f\"Input chunk lengths: {(outputs['length'])}\")\n",
    "print(f\"Chunk mapping: {outputs['overflow_to_sample_mapping']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {
    "id": "X1AhgViOwa1V",
    "outputId": "bc269d4d-b88c-4954-edae-a60f3af58429"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "context_length = 128\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"/workspace/code-search-net-tokenizer\",trust_remote_host=True)\n",
    "\n",
    "outputs = tokenizer(\n",
    "    #raw_datasets[\"train\"][:2][\"content\n",
    "    batch['text'][0],\n",
    "    truncation=True,\n",
    "    max_length=context_length,\n",
    "    return_overflowing_tokens=True,\n",
    "    return_length=True,\n",
    ")\n",
    "\n",
    "print(f\"Input IDs length: {len(outputs['input_ids'])}\")\n",
    "print(f\"Input chunk lengths: {(outputs['length'])}\")\n",
    "print(f\"Chunk mapping: {outputs['overflow_to_sample_mapping']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from torch.utils.data import IterableDataset\n",
    "\n",
    "# 加载分词器\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"/workspace/code-search-net-tokenizer\",trust_remote_host=True)\n",
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "\n",
    "# 定义分词函数\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['text'], padding=\"max_length\", truncation=True)\n",
    "\n",
    "# 创建分词后的 IterableDataset\n",
    "class TokenizedIterableDataset(IterableDataset):\n",
    "    def __init__(self, iterable_dataset):\n",
    "        self.iterable_dataset = iterable_dataset\n",
    "\n",
    "    def __iter__(self):\n",
    "        for sample in self.iterable_dataset:\n",
    "            tokenized_sample = tokenize_function(sample)\n",
    "            yield tokenized_sample\n",
    "    def __len__(self):\n",
    "        # 你可以返回一个固定的大小，或者 None\n",
    "        # 如果数据集的大小是未知的，返回 None\n",
    "        return len(self.iterable_dataset)\n",
    "\n",
    "\n",
    "# 创建分词后的数据集实例\n",
    "tokenized_datasets = TokenizedIterableDataset(iterable_dataset=dataset)\n",
    "tokenized_valid_datasets = TokenizedIterableDataset(iterable_dataset=valid_dataset)\n",
    "# 迭代分词后的数据集\n",
    "# for tokenized_sample in tokenized_datasets:\n",
    "#     print(tokenized_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [42, 14, 42, 14, 393, 6869, 516, 734, 2578, 5335, 8898, 18190, 269, 20473, 755, 4321, 26, 25890, 7766, 410, 56, 7, 173, 6651, 15, 1580, 15, 12237, 6374, 26, 2137, 6896, 2162, 17295, 438, 979, 3086, 12, 11991, 173, 2096, 302, 311, 256, 1367, 14, 42, 14, 173, 3061, 46431, 31316, 4568, 89, 173, 11643, 26, 1163, 26, 543, 249, 14, 77, 14, 2162, 41229, 928, 8966, 292, 256, 42294, 26503, 12, 333, 20473, 755, 4321, 2, 309, 8826, 2652, 542, 256, 656, 1241, 69, 221, 1170, 296, 333, 40718, 410, 56, 2, 664, 371, 7372, 14, 4489, 12, 13088, 14, 173, 20473, 755, 4321, 26, 25890, 7766, 410, 56, 300, 11312, 296, 4110, 517, 1872, 36486, 4489, 12, 13088, 14, 22381, 14, 14493, 14, 1026, 15, 82, 2850, 81, 3794, 40, 56, 7288, 173, 49268, 41419, 755, 4321, 308, 32, 5900, 87, 4321, 9, 1427, 39678, 3086, 12, 11991, 173, 2096, 11322, 1442, 461, 1367, 14, 42, 14, 393, 6869, 516, 1184, 633, 20660, 256, 643, 562, 311, 333, 20473, 755, 4321, 2, 802, 516, 461, 333, 2096, 11322, 393, 910, 731, 83, 7204, 914, 2498, 4492, 30749, 300, 30335, 266, 39019, 1504, 14, 173, 4053, 1872, 303, 760, 11687, 517, 329, 44608, 12, 350, 3963, 292, 231, 6201, 4110, 517, 41419, 55, 4321, 14, 1026, 12, 393, 6869, 516, 664, 302, 292, 1876, 350, 2567, 333, 20473, 755, 4321, 26, 25890, 7766, 410, 56, 2359, 507, 4743, 5563, 26, 173, 33, 2369, 5551, 585, 41419, 755, 4321, 308, 32, 5900, 87, 4321, 9, 517, 438, 979, 3086, 12, 11991, 815, 2624, 26, 5373, 331, 473, 11595, 173, 15106, 4492, 30749, 29925, 1338, 301, 13155, 461, 20717, 333, 40718, 410, 56, 2, 42294, 3773, 219, 329, 4825, 17381, 12262, 551, 4279, 12, 432, 321, 1266, 542, 683, 2755, 8942, 47120, 223, 12, 8503, 300, 2567, 266, 333, 20473, 755, 4321, 26, 507, 14426, 1367, 42231, 7204, 3155, 2704, 1187, 5064, 68, 14, 10750, 1872, 303, 760, 10696, 542, 8942, 47120, 223, 27246, 339, 292, 2704, 256, 14429, 292, 2567, 14, 173, 3642, 393, 6869, 516, 7, 366, 15639, 12, 686, 10241, 352, 47696, 473, 456, 4613, 2020, 348, 239, 230, 2020, 230, 6339, 89, 23127, 12, 333, 5998, 269, 2096, 11322, 393, 910, 731, 83, 4721, 1367, 14, 42, 14, 21905, 6281, 649, 3032, 1054, 9452, 3817, 1348, 296, 12, 350, 410, 6896, 914, 2429, 4425, 542, 633, 300, 17274, 1504, 292, 3215, 555, 551, 3216, 3820, 2359, 173, 15106, 3222, 649, 40345, 253, 333, 16009, 393, 910, 731, 83, 7204, 649, 3962, 5193, 2429, 4425, 2891, 577, 12, 4236, 14, 1890, 2302, 23188, 2469, 896, 577, 333, 2096, 1604, 311, 256, 1367, 14, 42, 2359, 173, 9339, 1890, 221, 554, 16664, 89, 238, 5708, 1122, 4017, 30638, 24455, 35518, 556, 5211, 785, 12089, 975, 928, 22003, 1186, 8942, 69, 785, 12089, 36512, 15459, 894, 975, 2431, 444, 1521, 319, 29871, 12, 19865, 785, 15611, 68, 9065, 8559, 8942, 12271, 13278, 219, 568, 13283, 563, 69, 6403, 3279, 5776, 9319, 6729, 2754, 865, 12939, 2294, 26, 269, 9721, 30641, 8677, 37053, 7, 2020, 225, 89, 3603, 459, 30159, 4907, 785, 12089, 975, 928, 22003, 1186, 447, 10039, 66, 3249, 8559, 393, 19737, 785, 930, 173, 7, 20473, 755, 4321, 7, 8571, 418, 35700, 15434, 173, 11285, 700, 407, 18588, 12, 865, 4457, 7936, 173, 4535, 20654, 41419, 755, 4321, 1367, 14, 42, 14, 393, 6869, 516, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n"
     ]
    }
   ],
   "source": [
    "flag = True\n",
    "for tokenized_sample in tokenized_datasets:\n",
    "    if flag:\n",
    "        print(tokenized_sample)\n",
    "    else:\n",
    "        continue\n",
    "    flag =False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/workspace/matmulfreellm/')    #先加入绝对路径，否则会报错，注意__file__表示的是当前执行文件的路径\n",
    "print(sys.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mmfreelm.models import HGRNBitConfig\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "# Config for the 370M model\n",
    "# Reference: https://huggingface.co/ridger/MMfreeLM-370M/blob/main/config.json\n",
    "config_params = {\n",
    "    \"attn_mode\": \"fused_recurrent\",\n",
    "    \"bos_token_id\": 1,\n",
    "    \"conv_size\": 4,\n",
    "    \"eos_token_id\": 2,\n",
    "    \"expand_ratio\": 1,\n",
    "    \"fuse_cross_entropy\": True,\n",
    "    \"hidden_act\": \"swish\",\n",
    "    \"hidden_ratio\": 4,\n",
    "    \"hidden_size\": 1024,\n",
    "    \"initializer_range\": 0.02,\n",
    "    \"intermediate_size\": None,\n",
    "    \"max_position_embeddings\": 2048,\n",
    "    \"model_type\": \"hgrn_bit\",\n",
    "    \"num_heads\": 1,\n",
    "    \"num_hidden_layers\": 24,\n",
    "    \"rms_norm_eps\": 1e-06,\n",
    "    \"share_conv_kernel\": True,\n",
    "    \"tie_word_embeddings\": False,\n",
    "    \"torch_dtype\": \"bfloat16\",\n",
    "    \"transformers_version\": \"4.40.2\",\n",
    "    \"use_cache\": True,\n",
    "    \"use_lower_bound\": True,\n",
    "    \"use_short_conv\": False,\n",
    "    \"vocab_size\": 32000,\n",
    "}\n",
    "\n",
    "config = HGRNBitConfig(**config_params)\n",
    "model = AutoModelForCausalLM.from_config(config)\n",
    "model_size = sum(t.numel() for t in model.parameters())\n",
    "print(f\"Matmul-free size: {model_size/1000**2:.1f}M parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {
    "id": "K7nBKtZlwa1W"
   },
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {
    "id": "zvZVk-Tewa1W",
    "outputId": "17c3a087-21f7-4e96-d21a-f74c5aad36bb"
   },
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "Subclasses of Dataset should implement __getitem__.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[260], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m out \u001b[38;5;241m=\u001b[39m data_collator([tokenized_datasets[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m5\u001b[39m)])\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m out:\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mout[key]\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[260], line 1\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[0;32m----> 1\u001b[0m out \u001b[38;5;241m=\u001b[39m data_collator([\u001b[43mtokenized_datasets\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m5\u001b[39m)])\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m out:\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mout[key]\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataset.py:63\u001b[0m, in \u001b[0;36mDataset.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, index) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T_co:\n\u001b[0;32m---> 63\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSubclasses of Dataset should implement __getitem__.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: Subclasses of Dataset should implement __getitem__."
     ]
    }
   ],
   "source": [
    "# out = data_collator([tokenized_datasets[i] for i in range(5)])\n",
    "# for key in out:\n",
    "#     print(f\"{key} shape: {out[key].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch._dynamo\n",
    "torch._dynamo.config.suppress_errors = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {
    "id": "euMAzuAywa1W"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[269], line 21\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Trainer, TrainingArguments\n\u001b[1;32m      3\u001b[0m args \u001b[38;5;241m=\u001b[39m TrainingArguments(\n\u001b[1;32m      4\u001b[0m     output_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmatmul_checkpoints\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      5\u001b[0m     per_device_train_batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     18\u001b[0m     push_to_hub\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     19\u001b[0m )\n\u001b[0;32m---> 21\u001b[0m trainer \u001b[38;5;241m=\u001b[39m \u001b[43mTrainer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_collator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_collator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenized_datasets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenized_valid_datasets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m#optimizers=\u001b[39;49;00m\n\u001b[1;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# train_dataset=tokenized_datasets[\"train\"],\u001b[39;49;00m\n\u001b[1;32m     30\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# eval_dataset=tokenized_datasets[\"valid\"],\u001b[39;49;00m\n\u001b[1;32m     31\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py:404\u001b[0m, in \u001b[0;36mTrainer.__init__\u001b[0;34m(self, model, args, data_collator, train_dataset, eval_dataset, tokenizer, model_init, compute_metrics, callbacks, optimizers, preprocess_logits_for_metrics)\u001b[0m\n\u001b[1;32m    402\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs \u001b[38;5;241m=\u001b[39m args\n\u001b[1;32m    403\u001b[0m \u001b[38;5;66;03m# Seed must be set before instantiating the model when using model\u001b[39;00m\n\u001b[0;32m--> 404\u001b[0m enable_full_determinism(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mseed) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mfull_determinism \u001b[38;5;28;01melse\u001b[39;00m \u001b[43mset_seed\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mseed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    405\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhp_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    406\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdeepspeed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer_utils.py:104\u001b[0m, in \u001b[0;36mset_seed\u001b[0;34m(seed, deterministic)\u001b[0m\n\u001b[1;32m    102\u001b[0m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mseed(seed)\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_torch_available():\n\u001b[0;32m--> 104\u001b[0m     \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmanual_seed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    105\u001b[0m     torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mmanual_seed_all(seed)\n\u001b[1;32m    106\u001b[0m     \u001b[38;5;66;03m# ^^ safe to call this function even if cuda is not available\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_compile.py:31\u001b[0m, in \u001b[0;36m_disable_dynamo.<locals>.inner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     28\u001b[0m     disable_fn \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mdisable(fn, recursive)\n\u001b[1;32m     29\u001b[0m     fn\u001b[38;5;241m.\u001b[39m__dynamo_disable \u001b[38;5;241m=\u001b[39m disable_fn\n\u001b[0;32m---> 31\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdisable_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:600\u001b[0m, in \u001b[0;36mDisableContext.__call__.<locals>._fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    598\u001b[0m prior \u001b[38;5;241m=\u001b[39m set_eval_frame(callback)\n\u001b[1;32m    599\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 600\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    601\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    602\u001b[0m     set_eval_frame(prior)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/random.py:46\u001b[0m, in \u001b[0;36mmanual_seed\u001b[0;34m(seed)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcuda\u001b[39;00m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39m_is_in_bad_fork():\n\u001b[0;32m---> 46\u001b[0m     \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmanual_seed_all\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmps\u001b[39;00m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mmps\u001b[38;5;241m.\u001b[39m_is_in_bad_fork():\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/cuda/random.py:127\u001b[0m, in \u001b[0;36mmanual_seed_all\u001b[0;34m(seed)\u001b[0m\n\u001b[1;32m    124\u001b[0m         default_generator \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mdefault_generators[i]\n\u001b[1;32m    125\u001b[0m         default_generator\u001b[38;5;241m.\u001b[39mmanual_seed(seed)\n\u001b[0;32m--> 127\u001b[0m \u001b[43m_lazy_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed_all\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/cuda/__init__.py:244\u001b[0m, in \u001b[0;36m_lazy_call\u001b[0;34m(callable, **kwargs)\u001b[0m\n\u001b[1;32m    242\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_lazy_call\u001b[39m(\u001b[38;5;28mcallable\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    243\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_initialized():\n\u001b[0;32m--> 244\u001b[0m         \u001b[38;5;28;43mcallable\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    245\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    246\u001b[0m         \u001b[38;5;66;03m# TODO(torch_deploy): this accesses linecache, which attempts to read the\u001b[39;00m\n\u001b[1;32m    247\u001b[0m         \u001b[38;5;66;03m# file system to get traceback info. Patch linecache or do something\u001b[39;00m\n\u001b[1;32m    248\u001b[0m         \u001b[38;5;66;03m# else here if this ends up being important.\u001b[39;00m\n\u001b[1;32m    249\u001b[0m         \u001b[38;5;28;01mglobal\u001b[39;00m _lazy_seed_tracker\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/cuda/random.py:125\u001b[0m, in \u001b[0;36mmanual_seed_all.<locals>.cb\u001b[0;34m()\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(device_count()):\n\u001b[1;32m    124\u001b[0m     default_generator \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mdefault_generators[i]\n\u001b[0;32m--> 125\u001b[0m     \u001b[43mdefault_generator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmanual_seed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseed\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"matmul_checkpoints\",\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=5_000,\n",
    "    logging_steps=5_000,\n",
    "    gradient_accumulation_steps=2,\n",
    "    num_train_epochs=1,\n",
    "    weight_decay=0.1,\n",
    "    warmup_steps=1_000,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    learning_rate=4e-3,\n",
    "    save_steps=5_000,\n",
    "    fp16=True,\n",
    "    push_to_hub=False,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    args=args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=tokenized_datasets,\n",
    "    eval_dataset=tokenized_valid_datasets,\n",
    "    #optimizers=\n",
    "    # train_dataset=tokenized_datasets[\"train\"],\n",
    "    # eval_dataset=tokenized_datasets[\"valid\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "os.environ['TORCH_USE_CUDA_DSA'] = '1'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {
    "id": "TTljDg-_wa1W"
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[246], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py:1938\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1936\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1937\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1938\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1939\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1940\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1941\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1942\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1943\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py:1948\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1945\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_inner_training_loop\u001b[39m(\n\u001b[1;32m   1946\u001b[0m     \u001b[38;5;28mself\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, args\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, resume_from_checkpoint\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, trial\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, ignore_keys_for_eval\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1947\u001b[0m ):\n\u001b[0;32m-> 1948\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfree_memory\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1949\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_train_batch_size \u001b[38;5;241m=\u001b[39m batch_size\n\u001b[1;32m   1950\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mauto_find_batch_size:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:3195\u001b[0m, in \u001b[0;36mAccelerator.free_memory\u001b[0;34m(self, *objects)\u001b[0m\n\u001b[1;32m   3193\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdeepspeed_engine_wrapped\u001b[38;5;241m.\u001b[39mengine\u001b[38;5;241m.\u001b[39mdestroy()\n\u001b[1;32m   3194\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdeepspeed_engine_wrapped \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 3195\u001b[0m objects \u001b[38;5;241m=\u001b[39m \u001b[43mrelease_memory\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mobjects\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3196\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_schedulers \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m   3197\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizers \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/utils/memory.py:77\u001b[0m, in \u001b[0;36mrelease_memory\u001b[0;34m(*objects)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(objects)):\n\u001b[1;32m     76\u001b[0m     objects[i] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m---> 77\u001b[0m \u001b[43mclear_device_cache\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgarbage_collection\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m objects\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/utils/memory.py:48\u001b[0m, in \u001b[0;36mclear_device_cache\u001b[0;34m(garbage_collection)\u001b[0m\n\u001b[1;32m     46\u001b[0m     torch\u001b[38;5;241m.\u001b[39mmps\u001b[38;5;241m.\u001b[39mempty_cache()\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 48\u001b[0m     \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mempty_cache\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/cuda/memory.py:170\u001b[0m, in \u001b[0;36mempty_cache\u001b[0;34m()\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Release all unoccupied cached memory currently held by the caching\u001b[39;00m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;124;03mallocator so that those can be used in other GPU application and visible in\u001b[39;00m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;124;03m`nvidia-smi`.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;124;03m    more details about GPU memory management.\u001b[39;00m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_initialized():\n\u001b[0;32m--> 170\u001b[0m     \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cuda_emptyCache\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qC9291Iuwa1W"
   },
   "outputs": [],
   "source": [
    "trainer.push_to_hub()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "brkU0Zctwa1W"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import pipeline\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "pipe = pipeline(\n",
    "    \"text-generation\", model=\"huggingface-course/codeparrot-ds\", device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7_-_joa9wa1W",
    "outputId": "265ee5db-ddf4-4c8e-872c-d46954d3d49c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "# create some data\n",
       "x = np.random.randn(100)\n",
       "y = np.random.randn(100)\n",
       "\n",
       "# create scatter plot with x, y\n",
       "plt.scatter(x, y)\n",
       "\n",
       "# create scatter"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt = \"\"\"\\\n",
    "# create some data\n",
    "x = np.random.randn(100)\n",
    "y = np.random.randn(100)\n",
    "\n",
    "# create scatter plot with x, y\n",
    "\"\"\"\n",
    "print(pipe(txt, num_return_sequences=1)[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "20VrF3mKwa1X",
    "outputId": "005aad84-3a5f-4317-8175-27735c6e5107"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "# create some data\n",
       "x = np.random.randn(100)\n",
       "y = np.random.randn(100)\n",
       "\n",
       "# create dataframe from x and y\n",
       "df = pd.DataFrame({'x': x, 'y': y})\n",
       "df.insert(0,'x', x)\n",
       "for"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt = \"\"\"\\\n",
    "# create some data\n",
    "x = np.random.randn(100)\n",
    "y = np.random.randn(100)\n",
    "\n",
    "# create dataframe from x and y\n",
    "\"\"\"\n",
    "print(pipe(txt, num_return_sequences=1)[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d58my5onwa1X",
    "outputId": "a120c623-5dbc-4f95-8201-5478a1fee7f2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "# dataframe with profession, income and name\n",
       "df = pd.DataFrame({'profession': x, 'income':y, 'name': z})\n",
       "\n",
       "# calculate the mean income per profession\n",
       "profession = df.groupby(['profession']).mean()\n",
       "\n",
       "# compute the"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt = \"\"\"\\\n",
    "# dataframe with profession, income and name\n",
    "df = pd.DataFrame({'profession': x, 'income':y, 'name': z})\n",
    "\n",
    "# calculate the mean income per profession\n",
    "\"\"\"\n",
    "print(pipe(txt, num_return_sequences=1)[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U2TOwrvJwa1X",
    "outputId": "3f9d3f17-b905-4429-bc0d-be892429a9e5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "# import random forest regressor from scikit-learn\n",
       "from sklearn.ensemble import RandomForestRegressor\n",
       "\n",
       "# fit random forest model with 300 estimators on X, y:\n",
       "rf = RandomForestRegressor(n_estimators=300, random_state=random_state, max_depth=3)\n",
       "rf.fit(X, y)\n",
       "rf"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt = \"\"\"\n",
    "# import random forest regressor from scikit-learn\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# fit random forest model with 300 estimators on X, y:\n",
    "\"\"\"\n",
    "print(pipe(txt, num_return_sequences=1)[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mkUxSaY4wa1X",
    "outputId": "85dff144-70b2-4579-e248-dc187a815ec6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Keyword has not single token: testtest'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keytoken_ids = []\n",
    "for keyword in [\n",
    "    \"plt\",\n",
    "    \"pd\",\n",
    "    \"sk\",\n",
    "    \"fit\",\n",
    "    \"predict\",\n",
    "    \" plt\",\n",
    "    \" pd\",\n",
    "    \" sk\",\n",
    "    \" fit\",\n",
    "    \" predict\",\n",
    "    \"testtest\",\n",
    "]:\n",
    "    ids = tokenizer([keyword]).input_ids[0]\n",
    "    if len(ids) == 1:\n",
    "        keytoken_ids.append(ids[0])\n",
    "    else:\n",
    "        print(f\"Keyword has not single token: {keyword}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TPz9u5_rwa1X"
   },
   "outputs": [],
   "source": [
    "from torch.nn import CrossEntropyLoss\n",
    "import torch\n",
    "\n",
    "\n",
    "def keytoken_weighted_loss(inputs, logits, keytoken_ids, alpha=1.0):\n",
    "    # Shift so that tokens < n predict n\n",
    "    shift_labels = inputs[..., 1:].contiguous()\n",
    "    shift_logits = logits[..., :-1, :].contiguous()\n",
    "    # Calculate per-token loss\n",
    "    loss_fct = CrossEntropyLoss(reduce=False)\n",
    "    loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
    "    # Resize and average loss per sample\n",
    "    loss_per_sample = loss.view(shift_logits.size(0), shift_logits.size(1)).mean(axis=1)\n",
    "    # Calculate and scale weighting\n",
    "    weights = torch.stack([(inputs == kt).float() for kt in keytoken_ids]).sum(\n",
    "        axis=[0, 2]\n",
    "    )\n",
    "    weights = alpha * (1.0 + weights)\n",
    "    # Calculate weighted average\n",
    "    weighted_loss = (loss_per_sample * weights).mean()\n",
    "    return weighted_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ztyq-IpOwa1X"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data.dataloader import DataLoader\n",
    "\n",
    "tokenized_dataset.set_format(\"torch\")\n",
    "train_dataloader = DataLoader(tokenized_dataset[\"train\"], batch_size=32, shuffle=True)\n",
    "eval_dataloader = DataLoader(tokenized_dataset[\"valid\"], batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1JrBJ9jFwa1X"
   },
   "outputs": [],
   "source": [
    "weight_decay = 0.1\n",
    "\n",
    "\n",
    "def get_grouped_params(model, no_decay=[\"bias\", \"LayerNorm.weight\"]):\n",
    "    params_with_wd, params_without_wd = [], []\n",
    "    for n, p in model.named_parameters():\n",
    "        if any(nd in n for nd in no_decay):\n",
    "            params_without_wd.append(p)\n",
    "        else:\n",
    "            params_with_wd.append(p)\n",
    "    return [\n",
    "        {\"params\": params_with_wd, \"weight_decay\": weight_decay},\n",
    "        {\"params\": params_without_wd, \"weight_decay\": 0.0},\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RyoYMb31wa1X"
   },
   "outputs": [],
   "source": [
    "def evaluate():\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    for step, batch in enumerate(eval_dataloader):\n",
    "        with torch.no_grad():\n",
    "            outputs = model(batch[\"input_ids\"], labels=batch[\"input_ids\"])\n",
    "\n",
    "        losses.append(accelerator.gather(outputs.loss))\n",
    "    loss = torch.mean(torch.cat(losses))\n",
    "    try:\n",
    "        perplexity = torch.exp(loss)\n",
    "    except OverflowError:\n",
    "        perplexity = float(\"inf\")\n",
    "    return loss.item(), perplexity.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_9m7QJkbwa1X"
   },
   "outputs": [],
   "source": [
    "model = GPT2LMHeadModel(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'STEOptimizer' from 'transformers' (/usr/local/lib/python3.10/dist-packages/transformers/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[262], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m STEOptimizer\n\u001b[1;32m      3\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m STEOptimizer(\n\u001b[1;32m      4\u001b[0m     model\u001b[38;5;241m.\u001b[39mparameters(),\n\u001b[1;32m      5\u001b[0m     lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-5\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     12\u001b[0m     anneal_fn\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m x: x  \u001b[38;5;66;03m# 退火函数，控制学习率随时间的变化\u001b[39;00m\n\u001b[1;32m     13\u001b[0m )\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'STEOptimizer' from 'transformers' (/usr/local/lib/python3.10/dist-packages/transformers/__init__.py)"
     ]
    }
   ],
   "source": [
    "from transformers import STEOptimizer\n",
    "\n",
    "optimizer = STEOptimizer(\n",
    "    model.parameters(),\n",
    "    lr=1e-5,\n",
    "    weight_decay=1e-2,\n",
    "    beta1=0.9,  # 类似于Adam优化器的超参数\n",
    "    beta2=0.999,  # 类似于Adam优化器的超参数\n",
    "    epsilon=1e-8,  # 避免分母为零\n",
    "    num_epochs=3,  # 训练的总轮数\n",
    "    dataloader=train_dataloader,  # 训练数据加载器\n",
    "    anneal_fn=lambda x: x  # 退火函数，控制学习率随时间的变化\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0HyXDXqCwa1X"
   },
   "outputs": [],
   "source": [
    "from torch.optim import AdamW\n",
    "\n",
    "optimizer = AdamW(get_grouped_params(model), lr=5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Uyhz_PBGwa1X"
   },
   "outputs": [],
   "source": [
    "from accelerate import Accelerator\n",
    "\n",
    "accelerator = Accelerator(fp16=True)\n",
    "\n",
    "model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(\n",
    "    model, optimizer, train_dataloader, eval_dataloader\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QtI6ZLRFwa1X"
   },
   "outputs": [],
   "source": [
    "from transformers import get_scheduler\n",
    "\n",
    "num_train_epochs = 1\n",
    "num_update_steps_per_epoch = len(train_dataloader)\n",
    "num_training_steps = num_train_epochs * num_update_steps_per_epoch\n",
    "\n",
    "lr_scheduler = get_scheduler(\n",
    "    name=\"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=1_000,\n",
    "    num_training_steps=num_training_steps,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nxbl4k2fwa1X",
    "outputId": "5e2c28f7-b523-49c5-9d7d-ea16ab48a57b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sgugger/codeparrot-ds-accelerate'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from huggingface_hub import Repository, get_full_repo_name\n",
    "\n",
    "model_name = \"codeparrot-ds-accelerate\"\n",
    "repo_name = get_full_repo_name(model_name)\n",
    "repo_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DchjYlmCwa1Y"
   },
   "outputs": [],
   "source": [
    "output_dir = \"codeparrot-ds-accelerate\"\n",
    "repo = Repository(output_dir, clone_from=repo_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZLU0Az1Nwa1b",
    "outputId": "c4bc6cc0-ce9e-4183-cd79-bbb540ccfc68"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10.934126853942871, 56057.14453125)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oy-z-FZGwa1b"
   },
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "gradient_accumulation_steps = 8\n",
    "eval_steps = 5_000\n",
    "\n",
    "model.train()\n",
    "completed_steps = 0\n",
    "for epoch in range(num_train_epochs):\n",
    "    for step, batch in tqdm(\n",
    "        enumerate(train_dataloader, start=1), total=num_training_steps\n",
    "    ):\n",
    "        logits = model(batch[\"input_ids\"]).logits\n",
    "        loss = keytoken_weighted_loss(batch[\"input_ids\"], logits, keytoken_ids)\n",
    "        if step % 100 == 0:\n",
    "            accelerator.print(\n",
    "                {\n",
    "                    \"lr\": get_lr(),\n",
    "                    \"samples\": step * samples_per_step,\n",
    "                    \"steps\": completed_steps,\n",
    "                    \"loss/train\": loss.item() * gradient_accumulation_steps,\n",
    "                }\n",
    "            )\n",
    "        loss = loss / gradient_accumulation_steps\n",
    "        accelerator.backward(loss)\n",
    "        if step % gradient_accumulation_steps == 0:\n",
    "            accelerator.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "            completed_steps += 1\n",
    "        if (step % (eval_steps * gradient_accumulation_steps)) == 0:\n",
    "            eval_loss, perplexity = evaluate()\n",
    "            accelerator.print({\"loss/eval\": eval_loss, \"perplexity\": perplexity})\n",
    "            model.train()\n",
    "            accelerator.wait_for_everyone()\n",
    "            unwrapped_model = accelerator.unwrap_model(model)\n",
    "            unwrapped_model.save_pretrained(output_dir, save_function=accelerator.save)\n",
    "            if accelerator.is_main_process:\n",
    "                tokenizer.save_pretrained(output_dir)\n",
    "                repo.push_to_hub(\n",
    "                    commit_message=f\"Training in progress step {step}\", blocking=False\n",
    "                )"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Training a causal language model from scratch (PyTorch)",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "matmulfree",
   "language": "python",
   "name": "matmulfree"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
